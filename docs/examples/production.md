# ç”Ÿç”¢ç’°å¢ƒç¯„ä¾‹

å±•ç¤ºåœ¨ç”Ÿç”¢ç’°å¢ƒä¸­éƒ¨ç½² Pretty-Loguru çš„æœ€ä½³å¯¦è¸ï¼ŒåŒ…æ‹¬æ€§èƒ½ç›£æ§ã€éŒ¯èª¤è¿½è¹¤å’Œæ—¥èªŒç®¡ç†ã€‚

## ç’°å¢ƒé…ç½®ç®¡ç†

æ ¹æ“šä¸åŒç’°å¢ƒä½¿ç”¨ä¸åŒçš„æ—¥èªŒé…ç½®ï¼š

```python
import os
from pretty_loguru import create_logger, ConfigTemplates, LoggerConfig

def get_environment_config() -> LoggerConfig:
    """æ ¹æ“šç’°å¢ƒè®Šæ•¸ç²å–é…ç½®"""
    env = os.getenv("ENVIRONMENT", "development").lower()
    
    configs = {
        "development": ConfigTemplates.development(),
        "testing": ConfigTemplates.testing(),
        "staging": LoggerConfig(
            level="INFO",
            log_path="logs/staging",
            rotation="100 MB",
            retention="14 days",
            compression="zip"
        ),
        "production": ConfigTemplates.production()
    }
    
    config = configs.get(env, ConfigTemplates.development())
    
    # ç’°å¢ƒç‰¹å®šè¦†å¯«
    if env == "production":
        # ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ JSON æ ¼å¼ä¾¿æ–¼æ—¥èªŒèšåˆ
        config.logger_format = '{"time":"{time}", "level":"{level}", "message":"{message}"}'
    
    return config

# ä½¿ç”¨ç’°å¢ƒé…ç½®
config = get_environment_config()
logger = create_logger("app", config=config)

# è¨˜éŒ„ç’°å¢ƒè³‡è¨Š
logger.info(f"Application started in {os.getenv('ENVIRONMENT', 'development')} mode")
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/06_production/deployment_logging.py)

## æ€§èƒ½ç›£æ§

ç›£æ§æ‡‰ç”¨ç¨‹åºæ€§èƒ½ä¸¦è¨˜éŒ„æŒ‡æ¨™ï¼š

```python
from pretty_loguru import create_logger
import time
import psutil
import asyncio
from functools import wraps

logger = create_logger("performance", log_path="logs/metrics")

def monitor_performance(func):
    """æ€§èƒ½ç›£æ§è£é£¾å™¨"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        try:
            result = await func(*args, **kwargs)
            status = "success"
        except Exception as e:
            result = None
            status = "error"
            raise
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            logger.info(
                f"Performance metrics",
                function=func.__name__,
                duration=f"{duration:.3f}s",
                memory_start=f"{start_memory:.1f}MB",
                memory_end=f"{end_memory:.1f}MB",
                memory_delta=f"{memory_delta:+.1f}MB",
                status=status
            )
            
            # è­¦å‘Šï¼šæ…¢é€Ÿæ“ä½œ
            if duration > 1.0:
                logger.warning(f"Slow operation detected: {func.__name__} took {duration:.3f}s")
            
            # è­¦å‘Šï¼šé«˜è¨˜æ†¶é«”ä½¿ç”¨
            if memory_delta > 100:
                logger.warning(f"High memory usage: {func.__name__} used {memory_delta:.1f}MB")
        
        return result
    
    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        # åŒæ­¥ç‰ˆæœ¬çš„ç›£æ§é‚è¼¯
        # ... (é¡ä¼¼å¯¦ç¾)
        pass
    
    return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

# ä½¿ç”¨ç¯„ä¾‹
@monitor_performance
async def process_large_dataset(size: int):
    """æ¨¡æ“¬å¤§æ•¸æ“šè™•ç†"""
    logger.info(f"Processing dataset of size {size}")
    
    # æ¨¡æ“¬è™•ç†
    data = list(range(size))
    await asyncio.sleep(0.5)  # æ¨¡æ“¬ I/O æ“ä½œ
    
    result = sum(data)
    logger.success(f"Processing complete. Result: {result}")
    return result

# ç³»çµ±ç›£æ§
class SystemMonitor:
    def __init__(self, logger):
        self.logger = logger
        self.running = False
    
    async def start(self):
        """é–‹å§‹ç›£æ§ç³»çµ±è³‡æº"""
        self.running = True
        while self.running:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # å®šæœŸè¨˜éŒ„ç³»çµ±ç‹€æ…‹
            self.logger.info(
                "System resources",
                cpu=f"{cpu_percent}%",
                memory_used=f"{memory.percent}%",
                memory_available=f"{memory.available / 1024**3:.1f}GB",
                disk_used=f"{disk.percent}%",
                disk_free=f"{disk.free / 1024**3:.1f}GB"
            )
            
            # è³‡æºè­¦å‘Š
            if cpu_percent > 80:
                self.logger.warning(f"High CPU usage: {cpu_percent}%")
            
            if memory.percent > 85:
                self.logger.warning(f"High memory usage: {memory.percent}%")
            
            if disk.percent > 90:
                self.logger.critical(f"Low disk space: {disk.free / 1024**3:.1f}GB free")
            
            await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/06_production/performance_monitoring.py)

## éŒ¯èª¤è¿½è¹¤

å®Œæ•´çš„éŒ¯èª¤è¿½è¹¤å’Œå ±å‘Šç³»çµ±ï¼š

```python
from pretty_loguru import create_logger
import traceback
import sys
from datetime import datetime
from typing import Dict, Any

logger = create_logger(
    "error_tracker",
    log_path="logs/errors",
    rotation="1 day",
    retention="30 days",
    level="INFO"
)

class ErrorTracker:
    def __init__(self):
        self.error_counts: Dict[str, int] = {}
        self.last_errors: Dict[str, datetime] = {}
    
    def track_error(self, error: Exception, context: Dict[str, Any] = None):
        """è¿½è¹¤å’Œè¨˜éŒ„éŒ¯èª¤"""
        error_type = type(error).__name__
        error_key = f"{error_type}:{str(error)}"
        
        # æ›´æ–°éŒ¯èª¤è¨ˆæ•¸
        self.error_counts[error_key] = self.error_counts.get(error_key, 0) + 1
        self.last_errors[error_key] = datetime.now()
        
        # è¨˜éŒ„éŒ¯èª¤è©³æƒ…
        logger.error(f"Error occurred: {error_type}")
        
        # è©³ç´°éŒ¯èª¤å ±å‘Š
        error_details = [
            f"Type: {error_type}",
            f"Message: {str(error)}",
            f"Occurrences: {self.error_counts[error_key]}",
            f"First seen: {self.last_errors.get(error_key)}",
        ]
        
        if context:
            error_details.append(f"Context: {context}")
        
        # æ·»åŠ å †ç–Šè¿½è¹¤
        tb = traceback.format_exc()
        error_details.extend([
            "Traceback:",
            *tb.split('\n')
        ])
        
        logger.block(
            f"âŒ Error Report - {error_type}",
            error_details,
            border_style="red",
            log_level="ERROR"
        )
        
        # é »ç¹éŒ¯èª¤è­¦å‘Š
        if self.error_counts[error_key] > 10:
            logger.critical(
                f"Frequent error detected: {error_key} "
                f"occurred {self.error_counts[error_key]} times"
            )
    
    def get_error_summary(self):
        """ç²å–éŒ¯èª¤æ‘˜è¦"""
        summary = []
        for error_key, count in sorted(
            self.error_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        ):
            last_seen = self.last_errors.get(error_key)
            summary.append(
                f"{error_key}: {count} times, "
                f"last: {last_seen.strftime('%Y-%m-%d %H:%M:%S')}"
            )
        
        logger.block(
            "ğŸ“Š Error Summary",
            summary[:10],  # Top 10 errors
            border_style="yellow",
            log_level="INFO"
        )
        
        return summary

# å…¨å±€éŒ¯èª¤è™•ç†å™¨
error_tracker = ErrorTracker()

def setup_global_error_handler():
    """è¨­ç½®å…¨å±€éŒ¯èª¤è™•ç†"""
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        
        error_tracker.track_error(
            exc_value,
            context={"type": "uncaught_exception"}
        )
    
    sys.excepthook = handle_exception

# ä½¿ç”¨ç¯„ä¾‹
def risky_operation(data):
    """å¯èƒ½å‡ºéŒ¯çš„æ“ä½œ"""
    try:
        # æŸäº›æ“ä½œ
        result = process_data(data)
        logger.success("Operation completed successfully")
        return result
    except ValueError as e:
        error_tracker.track_error(e, context={"data": data})
        raise
    except Exception as e:
        error_tracker.track_error(e, context={"data": data, "unexpected": True})
        raise
```

## æ—¥èªŒå£“ç¸®å’Œæ¸…ç†

è‡ªå‹•ç®¡ç†æ—¥èªŒæª”æ¡ˆå¤§å°ï¼š

```python
from pretty_loguru import create_logger
import os
import gzip
import shutil
from pathlib import Path

def custom_compression(file_path: str) -> str:
    """è‡ªå®šç¾©å£“ç¸®å‡½æ•¸"""
    gz_path = f"{file_path}.gz"
    
    # ä½¿ç”¨ gzip å£“ç¸®
    with open(file_path, 'rb') as f_in:
        with gzip.open(gz_path, 'wb', compresslevel=9) as f_out:
            shutil.copyfileobj(f_in, f_out)
    
    # åˆªé™¤åŸå§‹æª”æ¡ˆ
    os.remove(file_path)
    
    # è¨˜éŒ„å£“ç¸®è³‡è¨Š
    original_size = Path(file_path).stat().st_size
    compressed_size = Path(gz_path).stat().st_size
    ratio = (1 - compressed_size / original_size) * 100
    
    logger.info(
        f"Log compressed: {Path(file_path).name} "
        f"({original_size / 1024:.1f}KB â†’ {compressed_size / 1024:.1f}KB, "
        f"saved {ratio:.1f}%)"
    )
    
    return gz_path

# ä½¿ç”¨è‡ªå®šç¾©å£“ç¸®
logger = create_logger(
    "production",
    log_path="logs/prod",
    rotation="50 MB",
    retention="30 days",
    compression=custom_compression,
    start_cleaner=True  # å•Ÿç”¨è‡ªå‹•æ¸…ç†
)

# æ‰‹å‹•æ¸…ç†èˆŠæ—¥èªŒ
def cleanup_old_logs(log_dir: str, days: int = 30):
    """æ¸…ç†è¶…éæŒ‡å®šå¤©æ•¸çš„æ—¥èªŒ"""
    import time
    
    log_path = Path(log_dir)
    if not log_path.exists():
        return
    
    current_time = time.time()
    cutoff_time = current_time - (days * 24 * 60 * 60)
    
    cleaned_count = 0
    cleaned_size = 0
    
    for file_path in log_path.glob("*.log*"):
        if file_path.stat().st_mtime < cutoff_time:
            file_size = file_path.stat().st_size
            file_path.unlink()
            cleaned_count += 1
            cleaned_size += file_size
    
    if cleaned_count > 0:
        logger.info(
            f"Cleaned {cleaned_count} old log files, "
            f"freed {cleaned_size / 1024**2:.1f}MB"
        )
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/06_production/test_compression.py)

## éƒ¨ç½²æª¢æŸ¥æ¸…å–®

ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²å‰çš„æª¢æŸ¥ï¼š

```python
def production_checklist():
    """ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æª¢æŸ¥"""
    checks = []
    
    # æª¢æŸ¥æ—¥èªŒç›®éŒ„æ¬Šé™
    log_dir = Path("logs")
    if log_dir.exists() and os.access(log_dir, os.W_OK):
        checks.append("âœ… æ—¥èªŒç›®éŒ„å¯å¯«å…¥")
    else:
        checks.append("âŒ æ—¥èªŒç›®éŒ„ç„¡æ³•å¯«å…¥")
    
    # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
    disk_usage = psutil.disk_usage('/')
    if disk_usage.free > 1024**3:  # 1GB
        checks.append(f"âœ… ç£ç¢Ÿç©ºé–“å……è¶³ ({disk_usage.free / 1024**3:.1f}GB)")
    else:
        checks.append(f"âŒ ç£ç¢Ÿç©ºé–“ä¸è¶³ ({disk_usage.free / 1024**3:.1f}GB)")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    required_env = ["ENVIRONMENT", "LOG_LEVEL", "APP_VERSION"]
    for env_var in required_env:
        if os.getenv(env_var):
            checks.append(f"âœ… ç’°å¢ƒè®Šæ•¸ {env_var} å·²è¨­ç½®")
        else:
            checks.append(f"âŒ ç’°å¢ƒè®Šæ•¸ {env_var} æœªè¨­ç½®")
    
    # é¡¯ç¤ºæª¢æŸ¥çµæœ
    logger.block(
        "ğŸš€ ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æª¢æŸ¥",
        checks,
        border_style="blue",
        log_level="INFO"
    )
    
    return all("âœ…" in check for check in checks)
```

## ç›£æ§æ•´åˆ

èˆ‡ç›£æ§ç³»çµ±æ•´åˆï¼ˆå¦‚ Prometheusï¼‰ï¼š

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Prometheus æŒ‡æ¨™
log_counter = Counter('app_logs_total', 'Total log entries', ['level'])
error_counter = Counter('app_errors_total', 'Total errors', ['type'])
response_time = Histogram('app_response_time_seconds', 'Response time')

# åŒ…è£ logger ä»¥æ”¶é›†æŒ‡æ¨™
class MonitoredLogger:
    def __init__(self, logger):
        self.logger = logger
    
    def info(self, message, **kwargs):
        log_counter.labels(level='info').inc()
        self.logger.info(message, **kwargs)
    
    def error(self, message, error_type=None, **kwargs):
        log_counter.labels(level='error').inc()
        if error_type:
            error_counter.labels(type=error_type).inc()
        self.logger.error(message, **kwargs)
    
    @response_time.time()
    def log_with_timing(self, message, **kwargs):
        self.logger.info(message, **kwargs)

# ä½¿ç”¨ç›£æ§åŒ…è£çš„ logger
base_logger = create_logger("monitored_app")
logger = MonitoredLogger(base_logger)
```

## ä¸‹ä¸€æ­¥

- [é€²éšåŠŸèƒ½](./advanced.md) - è‡ªå®šç¾©å’Œæ“´å±•åŠŸèƒ½
- [ä¼æ¥­ç´šæ‡‰ç”¨](./enterprise.md) - å¤§è¦æ¨¡éƒ¨ç½²
- [é…ç½®ç®¡ç†](./configuration.md) - æ·±å…¥é…ç½®é¸é …