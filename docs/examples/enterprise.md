# ä¼æ¥­ç´šæ‡‰ç”¨ç¯„ä¾‹

å±•ç¤ºåœ¨ä¼æ¥­ç´šç’°å¢ƒä¸­éƒ¨ç½² Pretty-Loguru çš„æœ€ä½³å¯¦è¸ï¼ŒåŒ…æ‹¬å¾®æœå‹™æ¶æ§‹ã€å®‰å…¨æ€§ã€åˆè¦æ€§å’Œå¤§è¦æ¨¡æ—¥èªŒç®¡ç†ã€‚

## å¾®æœå‹™æ¶æ§‹

åœ¨å¾®æœå‹™ç’°å¢ƒä¸­çµ±ä¸€æ—¥èªŒç®¡ç†ï¼š

```python
from pretty_loguru import create_logger, LoggerConfig
import os
import socket
from typing import Optional
import uuid

class MicroserviceLogger:
    """å¾®æœå‹™å°ˆç”¨ Logger"""
    
    def __init__(
        self,
        service_name: str,
        service_version: str,
        environment: str = "production"
    ):
        self.service_name = service_name
        self.service_version = service_version
        self.environment = environment
        self.instance_id = self._generate_instance_id()
        
        # å‰µå»ºé…ç½®
        config = LoggerConfig(
            level=os.getenv("LOG_LEVEL", "INFO"),
            log_path=f"logs/{service_name}",
            rotation="100 MB",
            retention="30 days",
            compression="zip"
        )
        
        # å‰µå»º logger
        self.logger = create_logger(service_name, config=config)
        
        # ç¶å®šæœå‹™è³‡è¨Š
        self.logger = self.logger.bind(
            service=service_name,
            version=service_version,
            environment=environment,
            instance_id=self.instance_id,
            hostname=socket.gethostname()
        )
    
    def _generate_instance_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„å¯¦ä¾‹ ID"""
        return f"{self.service_name}-{uuid.uuid4().hex[:8]}"
    
    def log_request(self, request_id: str, method: str, path: str):
        """è¨˜éŒ„è«‹æ±‚"""
        return self.logger.bind(
            request_id=request_id,
            method=method,
            path=path
        )
    
    def log_response(self, request_id: str, status_code: int, duration: float):
        """è¨˜éŒ„éŸ¿æ‡‰"""
        self.logger.bind(request_id=request_id).info(
            f"Response: {status_code} ({duration:.3f}s)",
            status_code=status_code,
            duration=duration
        )
    
    def log_inter_service_call(
        self,
        target_service: str,
        operation: str,
        request_id: str
    ):
        """è¨˜éŒ„æœå‹™é–“èª¿ç”¨"""
        self.logger.bind(
            request_id=request_id,
            target_service=target_service,
            operation=operation
        ).info(f"Calling {target_service}.{operation}")

# ä½¿ç”¨ç¯„ä¾‹
# ç”¨æˆ¶æœå‹™
user_service = MicroserviceLogger(
    service_name="user-service",
    service_version="1.2.0",
    environment="production"
)

# è¨‚å–®æœå‹™
order_service = MicroserviceLogger(
    service_name="order-service",
    service_version="2.1.0",
    environment="production"
)

# è¨˜éŒ„æœå‹™é–“é€šä¿¡
request_id = str(uuid.uuid4())
order_service.log_inter_service_call(
    target_service="user-service",
    operation="get_user_details",
    request_id=request_id
)
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/08_enterprise/microservices_logging.py)

## å®‰å…¨æ—¥èªŒ

å¯¦æ–½å®‰å…¨æ—¥èªŒè¨˜éŒ„å’Œæ•æ„Ÿè³‡è¨Šä¿è­·ï¼š

```python
import re
import hashlib
from typing import Any, Dict, List
from pretty_loguru import create_logger

class SecurityLogger:
    """å®‰å…¨æ—¥èªŒè¨˜éŒ„å™¨"""
    
    # æ•æ„Ÿè³‡æ–™æ¨¡å¼
    SENSITIVE_PATTERNS = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',
        'api_key': r'\b[A-Za-z0-9]{32,}\b',
        'password': r'password["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
    }
    
    def __init__(self, service_name: str):
        self.logger = create_logger(
            f"security_{service_name}",
            log_path="logs/security",
            level="INFO"
        )
        self.audit_logger = create_logger(
            f"audit_{service_name}",
            log_path="logs/audit",
            level="INFO",
            rotation="1 day",
            retention="365 days"  # å¯©è¨ˆæ—¥èªŒä¿ç•™ä¸€å¹´
        )
    
    def sanitize_data(self, data: Any) -> Any:
        """æ¸…ç†æ•æ„Ÿè³‡æ–™"""
        if isinstance(data, str):
            return self._sanitize_string(data)
        elif isinstance(data, dict):
            return {k: self.sanitize_data(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.sanitize_data(item) for item in data]
        else:
            return data
    
    def _sanitize_string(self, text: str) -> str:
        """æ¸…ç†å­—ä¸²ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        for pattern_name, pattern in self.SENSITIVE_PATTERNS.items():
            text = re.sub(
                pattern,
                f"[REDACTED-{pattern_name.upper()}]",
                text,
                flags=re.IGNORECASE
            )
        return text
    
    def log_security_event(
        self,
        event_type: str,
        user_id: Optional[str] = None,
        details: Optional[Dict] = None,
        severity: str = "INFO"
    ):
        """è¨˜éŒ„å®‰å…¨äº‹ä»¶"""
        # æ¸…ç†æ•æ„Ÿè³‡æ–™
        safe_details = self.sanitize_data(details) if details else {}
        
        # è¨˜éŒ„åˆ°å®‰å…¨æ—¥èªŒ
        log_method = getattr(self.logger, severity.lower())
        log_method(
            f"Security Event: {event_type}",
            event_type=event_type,
            user_id=user_id,
            details=safe_details
        )
        
        # åŒæ™‚è¨˜éŒ„åˆ°å¯©è¨ˆæ—¥èªŒ
        self.audit_logger.info(
            f"AUDIT: {event_type}",
            event_type=event_type,
            user_id=user_id,
            user_hash=hashlib.sha256(str(user_id).encode()).hexdigest() if user_id else None,
            timestamp=time.time()
        )
    
    def log_authentication(
        self,
        user_id: str,
        success: bool,
        method: str,
        ip_address: str
    ):
        """è¨˜éŒ„èªè­‰äº‹ä»¶"""
        event_type = "AUTH_SUCCESS" if success else "AUTH_FAILURE"
        severity = "INFO" if success else "WARNING"
        
        self.log_security_event(
            event_type=event_type,
            user_id=user_id,
            details={
                "method": method,
                "ip_address": ip_address,
                "success": success
            },
            severity=severity
        )
    
    def log_authorization(
        self,
        user_id: str,
        resource: str,
        action: str,
        allowed: bool
    ):
        """è¨˜éŒ„æˆæ¬Šäº‹ä»¶"""
        event_type = "AUTHZ_GRANTED" if allowed else "AUTHZ_DENIED"
        severity = "INFO" if allowed else "WARNING"
        
        self.log_security_event(
            event_type=event_type,
            user_id=user_id,
            details={
                "resource": resource,
                "action": action,
                "allowed": allowed
            },
            severity=severity
        )

# ä½¿ç”¨ç¯„ä¾‹
security = SecurityLogger("api-gateway")

# è¨˜éŒ„èªè­‰
security.log_authentication(
    user_id="user123",
    success=True,
    method="oauth2",
    ip_address="192.168.1.100"
)

# è¨˜éŒ„æˆæ¬Š
security.log_authorization(
    user_id="user123",
    resource="/api/admin",
    action="DELETE",
    allowed=False
)

# è‡ªå‹•æ¸…ç†æ•æ„Ÿè³‡è¨Š
sensitive_data = {
    "user_email": "john@example.com",
    "phone": "123-456-7890",
    "api_key": "sk_test_abcdef123456789",
    "message": "User password: secretpass123"
}
security.logger.info("User data", data=security.sanitize_data(sensitive_data))
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/08_enterprise/security_logging.py)

## åˆè¦æ€§ç®¡ç†

æ»¿è¶³åˆè¦è¦æ±‚çš„æ—¥èªŒç®¡ç†ï¼š

```python
from pretty_loguru import create_logger
from datetime import datetime, timedelta
import json
from cryptography.fernet import Fernet

class ComplianceLogger:
    """åˆè¦æ€§æ—¥èªŒç®¡ç†å™¨"""
    
    def __init__(self, service_name: str, compliance_standards: List[str]):
        self.service_name = service_name
        self.compliance_standards = compliance_standards
        
        # æ ¹æ“šåˆè¦æ¨™æº–é…ç½®
        config = self._get_compliance_config()
        self.logger = create_logger(f"compliance_{service_name}", config=config)
        
        # åŠ å¯†é‡‘é‘°ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­æ‡‰å®‰å…¨å­˜å„²ï¼‰
        self.encryption_key = Fernet.generate_key()
        self.cipher = Fernet(self.encryption_key)
    
    def _get_compliance_config(self) -> LoggerConfig:
        """æ ¹æ“šåˆè¦æ¨™æº–ç²å–é…ç½®"""
        config = LoggerConfig(
            level="INFO",
            log_path="logs/compliance",
            rotation="1 day"
        )
        
        # GDPR è¦æ±‚
        if "GDPR" in self.compliance_standards:
            config.retention = "90 days"  # è³‡æ–™æœ€å°åŒ–åŸå‰‡
            
        # HIPAA è¦æ±‚
        if "HIPAA" in self.compliance_standards:
            config.retention = "6 years"  # é†«ç™‚è¨˜éŒ„ä¿ç•™è¦æ±‚
            
        # PCI-DSS è¦æ±‚
        if "PCI-DSS" in self.compliance_standards:
            config.retention = "1 year"
            config.compression = "zip"  # ç¯€çœå­˜å„²ç©ºé–“
        
        return config
    
    def log_data_access(
        self,
        user_id: str,
        data_type: str,
        operation: str,
        purpose: str,
        lawful_basis: Optional[str] = None
    ):
        """è¨˜éŒ„è³‡æ–™å­˜å–ï¼ˆGDPR è¦æ±‚ï¼‰"""
        self.logger.info(
            "Data Access",
            user_id=user_id,
            data_type=data_type,
            operation=operation,
            purpose=purpose,
            lawful_basis=lawful_basis,
            timestamp=datetime.utcnow().isoformat()
        )
    
    def log_consent(
        self,
        user_id: str,
        consent_type: str,
        granted: bool,
        version: str
    ):
        """è¨˜éŒ„ç”¨æˆ¶åŒæ„ï¼ˆGDPR è¦æ±‚ï¼‰"""
        self.logger.info(
            "User Consent",
            user_id=user_id,
            consent_type=consent_type,
            granted=granted,
            consent_version=version,
            timestamp=datetime.utcnow().isoformat()
        )
    
    def log_data_retention(
        self,
        data_type: str,
        records_processed: int,
        action: str
    ):
        """è¨˜éŒ„è³‡æ–™ä¿ç•™æ´»å‹•"""
        self.logger.info(
            "Data Retention Activity",
            data_type=data_type,
            records_processed=records_processed,
            action=action,
            compliance_standards=self.compliance_standards
        )
    
    def encrypt_sensitive_log(self, message: str, data: Dict) -> str:
        """åŠ å¯†æ•æ„Ÿæ—¥èªŒè³‡æ–™"""
        # å°‡è³‡æ–™è½‰æ›ç‚º JSON
        json_data = json.dumps(data)
        
        # åŠ å¯†
        encrypted = self.cipher.encrypt(json_data.encode())
        
        # è¨˜éŒ„åŠ å¯†çš„è³‡æ–™
        self.logger.info(
            message,
            encrypted_data=encrypted.decode(),
            encryption_method="Fernet"
        )
        
        return encrypted.decode()
    
    def generate_compliance_report(self):
        """ç”Ÿæˆåˆè¦å ±å‘Š"""
        report = {
            "service": self.service_name,
            "compliance_standards": self.compliance_standards,
            "report_date": datetime.utcnow().isoformat(),
            "log_retention": self._get_retention_policy(),
            "encryption_enabled": True,
            "audit_trail": "Enabled",
            "data_minimization": "GDPR" in self.compliance_standards
        }
        
        self.logger.block(
            "ğŸ“Š åˆè¦æ€§å ±å‘Š",
            [
                f"æœå‹™: {report['service']}",
                f"æ¨™æº–: {', '.join(report['compliance_standards'])}",
                f"å ±å‘Šæ—¥æœŸ: {report['report_date']}",
                f"æ—¥èªŒä¿ç•™: {report['log_retention']}",
                f"åŠ å¯†: {'å•Ÿç”¨' if report['encryption_enabled'] else 'ç¦ç”¨'}",
                f"å¯©è¨ˆè¿½è¹¤: {report['audit_trail']}",
                f"è³‡æ–™æœ€å°åŒ–: {'æ˜¯' if report['data_minimization'] else 'å¦'}"
            ],
            border_style="blue"
        )
        
        return report

# ä½¿ç”¨ç¯„ä¾‹
# GDPR åˆè¦
gdpr_logger = ComplianceLogger("user-service", ["GDPR"])
gdpr_logger.log_data_access(
    user_id="user123",
    data_type="personal_data",
    operation="READ",
    purpose="service_provision",
    lawful_basis="contract"
)

# HIPAA åˆè¦
hipaa_logger = ComplianceLogger("health-service", ["HIPAA"])
hipaa_logger.encrypt_sensitive_log(
    "Patient Record Access",
    {
        "patient_id": "P12345",
        "diagnosis": "Confidential",
        "treatment": "Confidential"
    }
)

# å¤šæ¨™æº–åˆè¦
multi_logger = ComplianceLogger("payment-service", ["PCI-DSS", "GDPR"])
multi_logger.generate_compliance_report()
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/08_enterprise/compliance.py)

## é›†ä¸­å¼æ—¥èªŒç®¡ç†

èˆ‡ ELK Stack æˆ–å…¶ä»–æ—¥èªŒèšåˆç³»çµ±æ•´åˆï¼š

```python
from pretty_loguru import create_logger
import requests
import json
from typing import List, Dict
import queue
import threading

class CentralizedLogger:
    """é›†ä¸­å¼æ—¥èªŒç®¡ç†å™¨"""
    
    def __init__(
        self,
        service_name: str,
        elasticsearch_url: str,
        buffer_size: int = 1000
    ):
        self.service_name = service_name
        self.es_url = elasticsearch_url
        self.buffer = queue.Queue(maxsize=buffer_size)
        
        # æœ¬åœ° logger
        self.logger = create_logger(
            service_name,
            log_path="logs/local",
            logger_format='{{
                "time": "{time:YYYY-MM-DD HH:mm:ss}",
                "level": "{level}",
                "service": "' + service_name + '",
                "message": "{message}",
                "extra": {extra}
            }}'
        )
        
        # å•Ÿå‹•èƒŒæ™¯å·¥ä½œç·šç¨‹
        self.worker_thread = threading.Thread(
            target=self._worker,
            daemon=True
        )
        self.worker_thread.start()
    
    def _worker(self):
        """èƒŒæ™¯å·¥ä½œç·šç¨‹ï¼Œè² è²¬ç™¼é€æ—¥èªŒåˆ° Elasticsearch"""
        batch = []
        while True:
            try:
                # æ”¶é›†æ‰¹é‡æ—¥èªŒ
                log_entry = self.buffer.get(timeout=1)
                batch.append(log_entry)
                
                # æ‰¹é‡ç™¼é€
                if len(batch) >= 100 or self.buffer.empty():
                    self._send_to_elasticsearch(batch)
                    batch = []
                    
            except queue.Empty:
                if batch:
                    self._send_to_elasticsearch(batch)
                    batch = []
            except Exception as e:
                self.logger.error(f"Log worker error: {e}")
    
    def _send_to_elasticsearch(self, logs: List[Dict]):
        """ç™¼é€æ—¥èªŒåˆ° Elasticsearch"""
        if not logs:
            return
        
        # æº–å‚™æ‰¹é‡è«‹æ±‚
        bulk_data = []
        for log in logs:
            # ç´¢å¼•å…ƒè³‡æ–™
            bulk_data.append(json.dumps({
                "index": {
                    "_index": f"logs-{self.service_name}",
                    "_type": "_doc"
                }
            }))
            # æ—¥èªŒè³‡æ–™
            bulk_data.append(json.dumps(log))
        
        # ç™¼é€è«‹æ±‚
        try:
            response = requests.post(
                f"{self.es_url}/_bulk",
                headers={"Content-Type": "application/x-ndjson"},
                data="\n".join(bulk_data) + "\n"
            )
            
            if response.status_code != 200:
                self.logger.error(
                    f"Failed to send logs to Elasticsearch: {response.status_code}"
                )
        except Exception as e:
            self.logger.error(f"Elasticsearch connection error: {e}")
            # å¯ä»¥å¯¦æ–½é‡è©¦é‚è¼¯æˆ–å°‡æ—¥èªŒå¯«å…¥æœ¬åœ°æª”æ¡ˆ
    
    def log(self, level: str, message: str, **extra):
        """è¨˜éŒ„æ—¥èªŒ"""
        # æœ¬åœ°è¨˜éŒ„
        getattr(self.logger, level)(message, **extra)
        
        # æº–å‚™ç™¼é€åˆ°é›†ä¸­å¼ç³»çµ±
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": self.service_name,
            "level": level.upper(),
            "message": message,
            **extra
        }
        
        # æ·»åŠ åˆ°ç·©è¡å€
        try:
            self.buffer.put_nowait(log_entry)
        except queue.Full:
            self.logger.warning("Log buffer full, dropping log entry")

# ç›£æ§æ•´åˆ
class MonitoringIntegration:
    """èˆ‡ç›£æ§ç³»çµ±æ•´åˆ"""
    
    def __init__(self, logger, prometheus_gateway: str):
        self.logger = logger
        self.prometheus_gateway = prometheus_gateway
        
        # Prometheus æŒ‡æ¨™
        from prometheus_client import CollectorRegistry, Counter, Histogram, push_to_gateway
        
        self.registry = CollectorRegistry()
        self.log_count = Counter(
            'app_logs_total',
            'Total number of log entries',
            ['service', 'level'],
            registry=self.registry
        )
        self.error_count = Counter(
            'app_errors_total',
            'Total number of errors',
            ['service', 'error_type'],
            registry=self.registry
        )
    
    def log_with_metrics(self, level: str, message: str, **kwargs):
        """è¨˜éŒ„æ—¥èªŒä¸¦æ›´æ–°æŒ‡æ¨™"""
        # è¨˜éŒ„æ—¥èªŒ
        self.logger.log(level, message, **kwargs)
        
        # æ›´æ–°æŒ‡æ¨™
        self.log_count.labels(
            service=self.logger.service_name,
            level=level
        ).inc()
        
        # å¦‚æœæ˜¯éŒ¯èª¤ï¼Œæ›´æ–°éŒ¯èª¤è¨ˆæ•¸
        if level in ["error", "critical"]:
            error_type = kwargs.get("error_type", "unknown")
            self.error_count.labels(
                service=self.logger.service_name,
                error_type=error_type
            ).inc()
        
        # æ¨é€æŒ‡æ¨™
        try:
            push_to_gateway(
                self.prometheus_gateway,
                job=self.logger.service_name,
                registry=self.registry
            )
        except Exception as e:
            self.logger.error(f"Failed to push metrics: {e}")

# ä½¿ç”¨ç¯„ä¾‹
# é›†ä¸­å¼æ—¥èªŒ
central_logger = CentralizedLogger(
    service_name="api-gateway",
    elasticsearch_url="http://localhost:9200"
)

# è¨˜éŒ„å„ç¨®ç´šåˆ¥çš„æ—¥èªŒ
central_logger.log("info", "Service started", version="1.0.0")
central_logger.log("warning", "High memory usage", memory_percent=85)
central_logger.log("error", "Database connection failed", 
    error_type="ConnectionError",
    database="users_db"
)
```

[æŸ¥çœ‹å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/JonesHong/pretty-loguru/blob/master/examples/08_enterprise/monitoring_integration.py)

## ç½é›£æ¢å¾©

å¯¦æ–½æ—¥èªŒå‚™ä»½å’Œæ¢å¾©ç­–ç•¥ï¼š

```python
import shutil
import tarfile
from pathlib import Path

class LogBackupManager:
    """æ—¥èªŒå‚™ä»½ç®¡ç†å™¨"""
    
    def __init__(self, log_dir: str, backup_dir: str):
        self.log_dir = Path(log_dir)
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = create_logger(
            "backup_manager",
            log_path="logs/backup"
        )
    
    def backup_logs(self, retention_days: int = 7):
        """å‚™ä»½æ—¥èªŒæª”æ¡ˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"logs_backup_{timestamp}.tar.gz"
        backup_path = self.backup_dir / backup_name
        
        try:
            # å‰µå»ºå£“ç¸®å‚™ä»½
            with tarfile.open(backup_path, "w:gz") as tar:
                tar.add(self.log_dir, arcname="logs")
            
            self.logger.success(
                f"Backup created: {backup_name}",
                size=f"{backup_path.stat().st_size / 1024**2:.1f}MB"
            )
            
            # æ¸…ç†èˆŠå‚™ä»½
            self._cleanup_old_backups(retention_days)
            
        except Exception as e:
            self.logger.error(f"Backup failed: {e}")
            raise
    
    def restore_logs(self, backup_file: str, target_dir: str):
        """æ¢å¾©æ—¥èªŒæª”æ¡ˆ"""
        backup_path = self.backup_dir / backup_file
        
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup file not found: {backup_file}")
        
        try:
            # è§£å£“å‚™ä»½
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(target_dir)
            
            self.logger.success(
                f"Logs restored from: {backup_file}",
                target=target_dir
            )
            
        except Exception as e:
            self.logger.error(f"Restore failed: {e}")
            raise
```

## ä¸‹ä¸€æ­¥

- [é…ç½®ç®¡ç†](./configuration.md) - æ·±å…¥äº†è§£é…ç½®é¸é …
- [ç”Ÿç”¢ç’°å¢ƒ](./production.md) - éƒ¨ç½²æœ€ä½³å¯¦è¸
- [API æ–‡æª”](../api/) - å®Œæ•´ API åƒè€ƒ