# æ•ˆèƒ½æœ€ä½³åŒ–

åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œæ—¥èªŒç³»çµ±çš„æ•ˆèƒ½ç›´æ¥å½±éŸ¿æ‡‰ç”¨ç¨‹å¼çš„æ•´é«”è¡¨ç¾ã€‚æœ¬æŒ‡å—å°‡å¹«åŠ©æ‚¨å„ªåŒ– pretty-loguru çš„æ•ˆèƒ½ã€‚

## âš¡ æ•ˆèƒ½åŸºæº–

### åŸºæº–æ¸¬è©¦

pretty-loguru åœ¨ä¸åŒå ´æ™¯ä¸‹çš„æ•ˆèƒ½è¡¨ç¾ï¼š

```python
import time
from pretty_loguru import create_logger

def benchmark_logging(logger, iterations=10000):
    """æ—¥èªŒæ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    start_time = time.time()
    
    for i in range(iterations):
        logger.info(f"æ¸¬è©¦è¨Šæ¯ {i}", extra={"iteration": i})
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"åŸ·è¡Œ {iterations} æ¬¡æ—¥èªŒæ“ä½œ")
    print(f"ç¸½æ™‚é–“: {duration:.2f} ç§’")
    print(f"å¹³å‡æ¯æ¬¡: {(duration/iterations)*1000:.3f} æ¯«ç§’")
    print(f"æ¯ç§’æ“ä½œæ•¸: {iterations/duration:.0f} ops/sec")

# æ¸¬è©¦ä¸åŒé…ç½®
console_logger = create_logger("console_only", sink="stdout")
file_logger = create_logger("file_only", log_path="logs/perf.log")
async_logger = create_logger("async", log_path="logs/async.log", enqueue=True)

print("æ§åˆ¶å°æ—¥èªŒæ•ˆèƒ½:")
benchmark_logging(console_logger)

print("\næª”æ¡ˆæ—¥èªŒæ•ˆèƒ½:")
benchmark_logging(file_logger)

print("\néåŒæ­¥æ—¥èªŒæ•ˆèƒ½:")
benchmark_logging(async_logger)
```

## ğŸš€ å„ªåŒ–ç­–ç•¥

### 1. éåŒæ­¥æ—¥èªŒè¨˜éŒ„

```python
# å•Ÿç”¨éåŒæ­¥è™•ç†ï¼Œæå‡é«˜ä½µç™¼æ•ˆèƒ½
async_logger = create_logger(
    name="async_app",
    log_path="logs/app.log",
    enqueue=True,        # å•Ÿç”¨éåŒæ­¥ä½‡åˆ—
    rotation="100 MB",
    compression="gz"
)

# æ‰¹æ¬¡è™•ç†æ—¥èªŒ
import asyncio

async def batch_logging():
    """æ‰¹æ¬¡æ—¥èªŒè¨˜éŒ„ç¯„ä¾‹"""
    tasks = []
    
    for i in range(1000):
        # å»ºç«‹æ—¥èªŒä»»å‹™ä½†ä¸ç«‹å³åŸ·è¡Œ
        task = asyncio.create_task(
            async_log_message(f"æ‰¹æ¬¡è¨Šæ¯ {i}")
        )
        tasks.append(task)
    
    # æ‰¹æ¬¡åŸ·è¡Œæ‰€æœ‰æ—¥èªŒä»»å‹™
    await asyncio.gather(*tasks)

async def async_log_message(message):
    """éåŒæ­¥æ—¥èªŒè¨˜éŒ„"""
    async_logger.info(message)

# åŸ·è¡Œæ‰¹æ¬¡æ—¥èªŒ
asyncio.run(batch_logging())
```

### 2. æ¢ä»¶å¼æ—¥èªŒè¨˜éŒ„

```python
# é¿å…ä¸å¿…è¦çš„å­—ä¸²æ ¼å¼åŒ–
def optimized_logging():
    logger = create_logger("optimized", level="INFO")
    
    # âŒ ä½æ•ˆï¼šç¸½æ˜¯åŸ·è¡Œå­—ä¸²æ ¼å¼åŒ–
    expensive_data = get_expensive_data()
    logger.debug(f"è©³ç´°è³‡æ–™: {expensive_data}")
    
    # âœ… é«˜æ•ˆï¼šåªåœ¨éœ€è¦æ™‚æ ¼å¼åŒ–
    if logger.level <= 10:  # DEBUG ç´šåˆ¥
        expensive_data = get_expensive_data()
        logger.debug(f"è©³ç´°è³‡æ–™: {expensive_data}")
    
    # âœ… æ›´å¥½ï¼šä½¿ç”¨å»¶é²æ ¼å¼åŒ–
    logger.debug("è©³ç´°è³‡æ–™: {data}", data=lambda: get_expensive_data())

def get_expensive_data():
    """æ¨¡æ“¬è€—æ™‚çš„è³‡æ–™è™•ç†"""
    time.sleep(0.1)
    return {"complex": "data structure"}
```

### 3. éæ¿¾å™¨å„ªåŒ–

```python
# é«˜æ•ˆçš„éæ¿¾å™¨è¨­è¨ˆ
def create_efficient_filter():
    """å»ºç«‹é«˜æ•ˆçš„æ—¥èªŒéæ¿¾å™¨"""
    
    # é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
    import re
    sensitive_pattern = re.compile(r'password|token|secret', re.IGNORECASE)
    
    def efficient_filter(record):
        # å¿«é€Ÿæª¢æŸ¥ï¼šé¿å…æ˜‚è²´çš„æ“ä½œ
        if record["level"].no < 20:  # ä½æ–¼ INFO ç´šåˆ¥
            return False
        
        # åªåœ¨å¿…è¦æ™‚é€²è¡Œæ¨¡å¼åŒ¹é…
        message = record.get("message", "")
        return not sensitive_pattern.search(message)
    
    return efficient_filter

# ä½¿ç”¨é«˜æ•ˆéæ¿¾å™¨
logger = create_logger(
    name="filtered",
    filter=create_efficient_filter()
)
```

### 4. è¨˜æ†¶é«”ç®¡ç†

```python
# æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨
memory_optimized_logger = create_logger(
    name="memory_optimized",
    log_path="logs/app.log",
    rotation="50 MB",        # è¼ƒå°çš„æª”æ¡ˆå¤§å°
    retention="7 days",      # è¼ƒçŸ­çš„ä¿ç•™æœŸ
    compression="gz",        # å•Ÿç”¨å£“ç¸®
    enqueue=True,           # éåŒæ­¥è™•ç†
    # æ§åˆ¶ä½‡åˆ—å¤§å°
    catch=True              # æ•ç²ä¾‹å¤–ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼
)

# å®šæœŸæ¸…ç†è³‡æº
import gc

def periodic_cleanup():
    """å®šæœŸæ¸…ç†è¨˜æ†¶é«”"""
    gc.collect()  # å¼·åˆ¶åƒåœ¾å›æ”¶
    print(f"è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œç•¶å‰ç‰©ä»¶æ•¸: {len(gc.get_objects())}")

# æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡æ¸…ç†
import threading
timer = threading.Timer(3600, periodic_cleanup)
timer.daemon = True
timer.start()
```

## ğŸ“Š ç›£æ§èˆ‡åˆ†æ

### æ•ˆèƒ½ç›£æ§

```python
import psutil
import time
from collections import deque

class PerformanceMonitor:
    """æ—¥èªŒæ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self, window_size=100):
        self.response_times = deque(maxlen=window_size)
        self.error_count = 0
        self.total_logs = 0
        self.start_time = time.time()
    
    def record_log_operation(self, duration, success=True):
        """è¨˜éŒ„æ—¥èªŒæ“ä½œ"""
        self.response_times.append(duration)
        self.total_logs += 1
        
        if not success:
            self.error_count += 1
    
    def get_metrics(self):
        """ç²å–æ•ˆèƒ½æŒ‡æ¨™"""
        if not self.response_times:
            return {}
        
        current_time = time.time()
        uptime = current_time - self.start_time
        
        return {
            "avg_response_time": sum(self.response_times) / len(self.response_times),
            "max_response_time": max(self.response_times),
            "min_response_time": min(self.response_times),
            "total_logs": self.total_logs,
            "error_rate": self.error_count / self.total_logs if self.total_logs > 0 else 0,
            "logs_per_second": self.total_logs / uptime if uptime > 0 else 0,
            "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024,
            "cpu_percent": psutil.Process().cpu_percent()
        }

# ä½¿ç”¨ç›£æ§å™¨
monitor = PerformanceMonitor()

def monitored_log_operation(logger, message):
    """ç›£æ§çš„æ—¥èªŒæ“ä½œ"""
    start_time = time.time()
    
    try:
        logger.info(message)
        success = True
    except Exception as e:
        success = False
        print(f"æ—¥èªŒéŒ¯èª¤: {e}")
    
    duration = time.time() - start_time
    monitor.record_log_operation(duration, success)

# å®šæœŸè¼¸å‡ºæ•ˆèƒ½å ±å‘Š
def print_performance_report():
    metrics = monitor.get_metrics()
    print("\nğŸ“Š æ—¥èªŒæ•ˆèƒ½å ±å‘Š:")
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")

# æ¯åˆ†é˜è¼¸å‡ºå ±å‘Š
import threading
def scheduled_report():
    print_performance_report()
    threading.Timer(60, scheduled_report).start()

scheduled_report()
```

### ç“¶é ¸åˆ†æ

```python
import cProfile
import pstats
from io import StringIO

def profile_logging_performance():
    """åˆ†ææ—¥èªŒæ•ˆèƒ½ç“¶é ¸"""
    
    pr = cProfile.Profile()
    pr.enable()
    
    # åŸ·è¡Œæ—¥èªŒæ“ä½œ
    logger = create_logger("profiling", log_path="logs/profile.log")
    
    for i in range(1000):
        logger.info(f"æ•ˆèƒ½åˆ†æè¨Šæ¯ {i}", extra={"data": {"key": f"value_{i}"}})
    
    pr.disable()
    
    # åˆ†æçµæœ
    s = StringIO()
    sortby = 'cumulative'
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
    ps.print_stats(20)  # é¡¯ç¤ºå‰ 20 å€‹æœ€æ…¢çš„å‡½æ•¸
    
    print("ğŸ” æ•ˆèƒ½åˆ†æçµæœ:")
    print(s.getvalue())

# åŸ·è¡Œæ•ˆèƒ½åˆ†æ
profile_logging_performance()
```

## ğŸ¯ ç‰¹å®šå ´æ™¯å„ªåŒ–

### é«˜ä½µç™¼ Web æ‡‰ç”¨

```python
# Web æ‡‰ç”¨å„ªåŒ–é…ç½®
web_logger = create_logger(
    name="web_app",
    log_path="logs/web.log",
    level="INFO",           # é¿å…éå¤š DEBUG è¨Šæ¯
    rotation="1 hour",      # é »ç¹è¼ªæ›é¿å…å–®æª”æ¡ˆéå¤§
    retention="24 hours",   # çŸ­æœŸä¿ç•™æ¸›å°‘ç£ç¢Ÿ I/O
    compression="gz",       # å£“ç¸®ç¯€çœç©ºé–“
    enqueue=True,          # éåŒæ­¥è™•ç†
    format="{time:HH:mm:ss} | {level} | {message}"  # ç°¡åŒ–æ ¼å¼
)

# è«‹æ±‚æ—¥èªŒå„ªåŒ–
def log_request(request_id, method, path, response_time):
    """å„ªåŒ–çš„è«‹æ±‚æ—¥èªŒ"""
    if response_time > 1.0:  # åªè¨˜éŒ„æ…¢è«‹æ±‚çš„è©³ç´°è³‡è¨Š
        web_logger.warning(
            f"æ…¢è«‹æ±‚: {method} {path}",
            extra={
                "request_id": request_id,
                "response_time": response_time,
                "type": "slow_request"
            }
        )
    else:
        web_logger.info(f"{method} {path}")
```

### å¾®æœå‹™æ¶æ§‹

```python
# å¾®æœå‹™æ—¥èªŒå„ªåŒ–
microservice_logger = create_logger(
    name="microservice",
    log_path="logs/service.log",
    level="INFO",
    rotation="100 MB",
    retention="30 days",
    compression="gz",
    enqueue=True,
    # çµæ§‹åŒ–æ ¼å¼ä¾¿æ–¼æ—¥èªŒèšåˆ
    serialize=True
)

def optimized_service_log(operation, service, duration, **kwargs):
    """å„ªåŒ–çš„å¾®æœå‹™æ—¥èªŒ"""
    log_data = {
        "operation": operation,
        "service": service,
        "duration_ms": round(duration * 1000, 2),
        **kwargs
    }
    
    # æ ¹æ“šè€—æ™‚æ±ºå®šæ—¥èªŒç´šåˆ¥
    if duration > 5.0:
        microservice_logger.error("æ“ä½œè¶…æ™‚", extra=log_data)
    elif duration > 1.0:
        microservice_logger.warning("æ“ä½œè¼ƒæ…¢", extra=log_data)
    else:
        microservice_logger.info("æ“ä½œå®Œæˆ", extra=log_data)
```

### æ‰¹æ¬¡è™•ç†ç³»çµ±

```python
# æ‰¹æ¬¡è™•ç†å„ªåŒ–
batch_logger = create_logger(
    name="batch_processing",
    log_path="logs/batch.log",
    level="INFO",
    rotation="daily",
    retention="90 days",
    compression="gz",
    # æ‰¹æ¬¡ç³»çµ±å¯ä»¥ä½¿ç”¨åŒæ­¥æ¨¡å¼
    enqueue=False
)

class BatchProgressLogger:
    """æ‰¹æ¬¡é€²åº¦æ—¥èªŒå™¨"""
    
    def __init__(self, total_items, log_interval=1000):
        self.total_items = total_items
        self.log_interval = log_interval
        self.processed = 0
        self.start_time = time.time()
    
    def log_progress(self, items_processed=1):
        """è¨˜éŒ„è™•ç†é€²åº¦"""
        self.processed += items_processed
        
        if self.processed % self.log_interval == 0:
            elapsed = time.time() - self.start_time
            rate = self.processed / elapsed if elapsed > 0 else 0
            remaining = (self.total_items - self.processed) / rate if rate > 0 else 0
            
            batch_logger.info(
                f"æ‰¹æ¬¡é€²åº¦: {self.processed}/{self.total_items}",
                extra={
                    "progress_percent": (self.processed / self.total_items) * 100,
                    "processing_rate": rate,
                    "estimated_remaining_seconds": remaining
                }
            )

# ä½¿ç”¨æ‰¹æ¬¡é€²åº¦è¨˜éŒ„å™¨
progress = BatchProgressLogger(total_items=10000)
for i in range(10000):
    # è™•ç†è³‡æ–™
    time.sleep(0.001)  # æ¨¡æ“¬è™•ç†æ™‚é–“
    progress.log_progress()
```

## âš™ï¸ ç³»çµ±ç´šå„ªåŒ–

### ä½œæ¥­ç³»çµ±è¨­å®š

```bash
# Linux ç³»çµ±å„ªåŒ–è¨­å®š

# å¢åŠ æª”æ¡ˆæè¿°ç¬¦é™åˆ¶
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf

# å„ªåŒ–ç£ç¢Ÿ I/O
echo "deadline" > /sys/block/sda/queue/scheduler

# èª¿æ•´ vm.dirty_ratio ä»¥å„ªåŒ–å¯«å…¥æ•ˆèƒ½
echo "vm.dirty_ratio = 15" >> /etc/sysctl.conf
echo "vm.dirty_background_ratio = 5" >> /etc/sysctl.conf
```

### Docker ç’°å¢ƒå„ªåŒ–

```dockerfile
# Dockerfile å„ªåŒ–
FROM python:3.11-slim

# å®‰è£ pretty-loguru
RUN pip install pretty-loguru

# å»ºç«‹æ—¥èªŒç›®éŒ„ä¸¦è¨­å®šæ¬Šé™
RUN mkdir -p /var/log/app && \
    chmod 755 /var/log/app

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO
ENV LOG_PATH=/var/log/app

# ä½¿ç”¨é root ç”¨æˆ¶
RUN useradd -m appuser
USER appuser

WORKDIR /app
COPY . .

CMD ["python", "app.py"]
```

```yaml
# docker-compose.yml å„ªåŒ–
version: '3.8'
services:
  app:
    build: .
    volumes:
      # ä½¿ç”¨ tmpfs æå‡æ—¥èªŒå¯«å…¥æ•ˆèƒ½
      - type: tmpfs
        target: /var/log/app
        tmpfs:
          size: 1G
    environment:
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
```

## ğŸ“ˆ æ•ˆèƒ½åŸºæº–èˆ‡ç›®æ¨™

### æ•ˆèƒ½ç›®æ¨™

| å ´æ™¯ | ç›®æ¨™æ•ˆèƒ½ | è¨˜æ†¶é«”ä½¿ç”¨ |
|------|----------|------------|
| ä½æµé‡æ‡‰ç”¨ | >1,000 logs/sec | <50MB |
| ä¸­æµé‡æ‡‰ç”¨ | >5,000 logs/sec | <100MB |
| é«˜æµé‡æ‡‰ç”¨ | >20,000 logs/sec | <200MB |

### ç›£æ§æŒ‡æ¨™

```python
# é—œéµæ•ˆèƒ½æŒ‡æ¨™ (KPI)
performance_kpis = {
    "avg_log_latency_ms": 1.0,      # å¹³å‡æ—¥èªŒå»¶é² < 1ms
    "p95_log_latency_ms": 5.0,      # 95% æ—¥èªŒå»¶é² < 5ms
    "memory_usage_mb": 100,         # è¨˜æ†¶é«”ä½¿ç”¨ < 100MB
    "cpu_usage_percent": 5,         # CPU ä½¿ç”¨ç‡ < 5%
    "disk_io_wait_percent": 10,     # ç£ç¢Ÿ I/O ç­‰å¾… < 10%
    "error_rate_percent": 0.1       # éŒ¯èª¤ç‡ < 0.1%
}

def validate_performance(current_metrics):
    """é©—è­‰æ•ˆèƒ½æ˜¯å¦ç¬¦åˆç›®æ¨™"""
    for metric, target in performance_kpis.items():
        current = current_metrics.get(metric, 0)
        if current > target:
            print(f"âš ï¸ æ•ˆèƒ½è­¦å‘Š: {metric} = {current}, ç›®æ¨™ < {target}")
        else:
            print(f"âœ… æ•ˆèƒ½æ­£å¸¸: {metric} = {current}")
```

## ğŸ”— ç›¸é—œè³‡æº

- [è‡ªå®šç¾©é…ç½®](./custom-config) - å®Œæ•´é…ç½®é¸é …
- [æ—¥èªŒè¼ªæ›](./log-rotation) - æª”æ¡ˆç®¡ç†ç­–ç•¥
- [ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²](./production) - ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸
- [ç¯„ä¾‹é›†åˆ](../examples/production/) - æ•ˆèƒ½å„ªåŒ–ç¯„ä¾‹