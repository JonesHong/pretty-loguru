# ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

å°‡ pretty-loguru éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒéœ€è¦è€ƒæ…®å¯é æ€§ã€æ•ˆèƒ½ã€å®‰å…¨æ€§å’Œå¯ç¶­è­·æ€§ã€‚æœ¬æŒ‡å—æä¾›ä¼æ¥­ç´šéƒ¨ç½²çš„æœ€ä½³å¯¦è¸ã€‚

## ğŸ—ï¸ éƒ¨ç½²æ¶æ§‹

### å–®æ©Ÿéƒ¨ç½²

```python
# production_config.py
import os
from pretty_loguru import create_logger

def create_production_logger(service_name):
    """å»ºç«‹ç”Ÿç”¢ç’°å¢ƒæ—¥èªŒé…ç½®"""
    
    # ç’°å¢ƒè®Šæ•¸é…ç½®
    log_level = os.getenv("LOG_LEVEL", "INFO")
    log_path = os.getenv("LOG_PATH", f"/var/log/{service_name}")
    environment = os.getenv("ENVIRONMENT", "production")
    
    # ç”Ÿç”¢ç’°å¢ƒé…ç½®
    logger = create_logger(
        name=f"{service_name}_{environment}",
        level=log_level,
        log_path=f"{log_path}/{service_name}.log",
        rotation="100 MB",          # é©ä¸­çš„æª”æ¡ˆå¤§å°
        retention="30 days",        # åˆè¦è¦æ±‚çš„ä¿ç•™æœŸ
        compression="gzip",         # å£“ç¸®ç¯€çœç©ºé–“
        enqueue=True,              # éåŒæ­¥è™•ç†æå‡æ•ˆèƒ½
        backtrace=False,           # ç”Ÿç”¢ç’°å¢ƒä¸éœ€è¦å®Œæ•´è¿½è¹¤
        diagnose=False,            # é—œé–‰è¨ºæ–·è³‡è¨Š
        catch=True,                # æ•ç²ä¾‹å¤–é¿å…å´©æ½°
        # ç”Ÿç”¢ç’°å¢ƒæ ¼å¼
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {process} | {name} | {function}:{line} - {message}",
        serialize=True             # JSON æ ¼å¼ä¾¿æ–¼æ—¥èªŒèšåˆ
    )
    
    return logger

# ä½¿ç”¨ç¯„ä¾‹
app_logger = create_production_logger("webapp")
db_logger = create_production_logger("database")
cache_logger = create_production_logger("redis")
```

### å¾®æœå‹™éƒ¨ç½²

```python
# microservice_logging.py
import os
import socket
from pretty_loguru import create_logger

class MicroserviceLogger:
    """å¾®æœå‹™æ—¥èªŒç®¡ç†å™¨"""
    
    def __init__(self, service_name, version="1.0.0"):
        self.service_name = service_name
        self.version = version
        self.instance_id = self._get_instance_id()
        self.logger = self._create_logger()
    
    def _get_instance_id(self):
        """ç²å–æœå‹™å¯¦ä¾‹ ID"""
        hostname = socket.gethostname()
        pod_name = os.getenv("POD_NAME", hostname)
        return f"{self.service_name}-{pod_name}"
    
    def _create_logger(self):
        """å»ºç«‹å¾®æœå‹™å°ˆç”¨æ—¥èªŒè¨˜éŒ„å™¨"""
        return create_logger(
            name=self.service_name,
            level=os.getenv("LOG_LEVEL", "INFO"),
            log_path=f"/var/log/{self.service_name}/{self.service_name}.log",
            rotation="daily",
            retention="7 days",
            compression="gzip",
            enqueue=True,
            # å¾®æœå‹™æ¨™æº–æ ¼å¼
            format='{"timestamp": "{time:YYYY-MM-DD HH:mm:ss.SSS}", "level": "{level}", "service": "' + self.service_name + '", "instance": "' + self.instance_id + '", "version": "' + self.version + '", "message": "{message}", "extra": {extra}}',
            serialize=True
        )
    
    def info(self, message, **kwargs):
        """è³‡è¨Šæ—¥èªŒ"""
        self.logger.info(message, extra=self._add_context(kwargs))
    
    def error(self, message, **kwargs):
        """éŒ¯èª¤æ—¥èªŒ"""
        self.logger.error(message, extra=self._add_context(kwargs))
    
    def warning(self, message, **kwargs):
        """è­¦å‘Šæ—¥èªŒ"""
        self.logger.warning(message, extra=self._add_context(kwargs))
    
    def _add_context(self, extra_data):
        """æ·»åŠ æœå‹™ä¸Šä¸‹æ–‡"""
        context = {
            "service": self.service_name,
            "instance": self.instance_id,
            "version": self.version,
            "environment": os.getenv("ENVIRONMENT", "production")
        }
        context.update(extra_data)
        return context

# ä½¿ç”¨å¾®æœå‹™æ—¥èªŒå™¨
logger = MicroserviceLogger("user-service", "2.1.0")
logger.info("æœå‹™å•Ÿå‹•", port=8080, health_check="/health")
```

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### Docker é…ç½®

```dockerfile
# Dockerfile
FROM python:3.11-slim

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    logrotate \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹æ‡‰ç”¨ç”¨æˆ¶
RUN useradd -m -u 1000 appuser

# å»ºç«‹æ—¥èªŒç›®éŒ„
RUN mkdir -p /var/log/app && \
    chown -R appuser:appuser /var/log/app

# å®‰è£ Python ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
WORKDIR /app
COPY --chown=appuser:appuser . .

# åˆ‡æ›åˆ°é root ç”¨æˆ¶
USER appuser

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8080/health')"

# å•Ÿå‹•æ‡‰ç”¨
EXPOSE 8080
CMD ["python", "app.py"]
```

### Kubernetes éƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: myapp:latest
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: ENVIRONMENT
          value: "production"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: logs
          mountPath: /var/log/app
        - name: config
          mountPath: /app/config
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: logs
        emptyDir: {}
      - name: config
        configMap:
          name: webapp-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  logging.json: |
    {
      "log_level": "INFO",
      "rotation": "100 MB",
      "retention": "7 days",
      "compression": "gzip"
    }
```

## ğŸ“Š ç›£æ§èˆ‡è§€æ¸¬

### æ—¥èªŒèšåˆ

```python
# log_aggregation.py
import json
from datetime import datetime
from pretty_loguru import create_logger

class LogAggregator:
    """æ—¥èªŒèšåˆå™¨ - èˆ‡ ELK Stack æ•´åˆ"""
    
    def __init__(self, service_name):
        self.service_name = service_name
        self.logger = create_logger(
            name=f"aggregated_{service_name}",
            level="INFO",
            log_path=f"/var/log/aggregated/{service_name}.log",
            rotation="hourly",
            retention="24 hours",
            # ELK å‹å¥½çš„æ ¼å¼
            format='{"@timestamp": "{time:YYYY-MM-DDTHH:mm:ss.SSSZ}", "@version": "1", "host": "{host}", "level": "{level}", "logger_name": "{name}", "thread": "{thread}", "message": "{message}", "fields": {extra}}',
            serialize=True
        )
    
    def log_request(self, method, path, status_code, response_time, user_id=None):
        """è¨˜éŒ„ HTTP è«‹æ±‚"""
        self.logger.info(
            f"{method} {path} - {status_code}",
            extra={
                "event_type": "http_request",
                "http": {
                    "method": method,
                    "path": path,
                    "status_code": status_code,
                    "response_time_ms": round(response_time * 1000, 2)
                },
                "user_id": user_id,
                "service": self.service_name
            }
        )
    
    def log_database_query(self, query_type, table, duration, rows_affected=None):
        """è¨˜éŒ„è³‡æ–™åº«æŸ¥è©¢"""
        self.logger.info(
            f"DB {query_type} on {table}",
            extra={
                "event_type": "database_query",
                "database": {
                    "query_type": query_type,
                    "table": table,
                    "duration_ms": round(duration * 1000, 2),
                    "rows_affected": rows_affected
                },
                "service": self.service_name
            }
        )
    
    def log_business_event(self, event_name, event_data=None):
        """è¨˜éŒ„æ¥­å‹™äº‹ä»¶"""
        self.logger.info(
            f"Business event: {event_name}",
            extra={
                "event_type": "business_event",
                "event_name": event_name,
                "event_data": event_data or {},
                "service": self.service_name
            }
        )

# ä½¿ç”¨æ—¥èªŒèšåˆå™¨
aggregator = LogAggregator("user-service")

# è¨˜éŒ„ä¸åŒé¡å‹çš„äº‹ä»¶
aggregator.log_request("GET", "/api/users/123", 200, 0.045, user_id="user_123")
aggregator.log_database_query("SELECT", "users", 0.023, rows_affected=1)
aggregator.log_business_event("user_login", {"login_method": "oauth", "provider": "google"})
```

### æŒ‡æ¨™æ”¶é›†

```python
# metrics_collection.py
import time
from collections import defaultdict, Counter
from pretty_loguru import create_logger

class MetricsCollector:
    """æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self):
        self.logger = create_logger(
            name="metrics",
            level="INFO",
            log_path="/var/log/metrics/metrics.log",
            rotation="hourly",
            format='{"timestamp": "{time:YYYY-MM-DDTHH:mm:ss.SSSZ}", "metric_type": "application", "data": {extra}}',
            serialize=True
        )
        
        # æŒ‡æ¨™å­˜å„²
        self.counters = defaultdict(int)
        self.gauges = defaultdict(float)
        self.histograms = defaultdict(list)
        self.timers = {}
    
    def increment_counter(self, name, labels=None, value=1):
        """å¢åŠ è¨ˆæ•¸å™¨"""
        key = f"{name}:{labels or {}}"
        self.counters[key] += value
        
        self.logger.info(
            f"Counter {name} incremented",
            extra={
                "metric_name": name,
                "metric_type": "counter",
                "value": value,
                "labels": labels or {},
                "total": self.counters[key]
            }
        )
    
    def set_gauge(self, name, value, labels=None):
        """è¨­å®šé‡è¡¨å€¼"""
        key = f"{name}:{labels or {}}"
        self.gauges[key] = value
        
        self.logger.info(
            f"Gauge {name} set",
            extra={
                "metric_name": name,
                "metric_type": "gauge",
                "value": value,
                "labels": labels or {}
            }
        )
    
    def record_histogram(self, name, value, labels=None):
        """è¨˜éŒ„ç›´æ–¹åœ–å€¼"""
        key = f"{name}:{labels or {}}"
        self.histograms[key].append(value)
        
        # è¨ˆç®—çµ±è¨ˆå€¼
        values = self.histograms[key]
        count = len(values)
        sum_val = sum(values)
        avg_val = sum_val / count
        
        self.logger.info(
            f"Histogram {name} recorded",
            extra={
                "metric_name": name,
                "metric_type": "histogram",
                "value": value,
                "count": count,
                "sum": sum_val,
                "average": avg_val,
                "labels": labels or {}
            }
        )
    
    def start_timer(self, name, labels=None):
        """é–‹å§‹è¨ˆæ™‚"""
        key = f"{name}:{labels or {}}"
        self.timers[key] = time.time()
    
    def end_timer(self, name, labels=None):
        """çµæŸè¨ˆæ™‚ä¸¦è¨˜éŒ„"""
        key = f"{name}:{labels or {}}"
        if key in self.timers:
            duration = time.time() - self.timers[key]
            del self.timers[key]
            self.record_histogram(f"{name}_duration", duration, labels)
            return duration
        return None

# ä½¿ç”¨æŒ‡æ¨™æ”¶é›†å™¨
metrics = MetricsCollector()

# è¨˜éŒ„å„ç¨®æŒ‡æ¨™
metrics.increment_counter("http_requests_total", {"method": "GET", "status": "200"})
metrics.set_gauge("active_connections", 42)
metrics.start_timer("request_processing", {"endpoint": "/api/users"})
time.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
metrics.end_timer("request_processing", {"endpoint": "/api/users"})
```

## ğŸ”’ å®‰å…¨é…ç½®

### æ—¥èªŒå®‰å…¨

```python
# secure_logging.py
import hashlib
import re
from pretty_loguru import create_logger

class SecureLogger:
    """å®‰å…¨æ—¥èªŒè¨˜éŒ„å™¨"""
    
    def __init__(self, service_name):
        self.service_name = service_name
        self.sensitive_patterns = [
            r'password\s*[=:]\s*["\']([^"\']+)["\']',
            r'token\s*[=:]\s*["\']([^"\']+)["\']',
            r'api[_-]?key\s*[=:]\s*["\']([^"\']+)["\']',
            r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',  # ä¿¡ç”¨å¡è™Ÿ
            r'\b\d{3}-\d{2}-\d{4}\b'  # SSN
        ]
        
        self.logger = create_logger(
            name=f"secure_{service_name}",
            level="INFO",
            log_path=f"/var/log/secure/{service_name}.log",
            rotation="daily",
            retention="90 days",  # å®‰å…¨æ—¥èªŒé•·æœŸä¿ç•™
            compression="gzip",
            # ç¢ºä¿æª”æ¡ˆæ¬Šé™å®‰å…¨
            enqueue=True
        )
    
    def sanitize_message(self, message):
        """æ¸…ç†æ•æ„Ÿè³‡è¨Š"""
        sanitized = message
        
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, r'***REDACTED***', sanitized, flags=re.IGNORECASE)
        
        return sanitized
    
    def hash_pii(self, data):
        """é›œæ¹Šå€‹äººè­˜åˆ¥è³‡è¨Š"""
        if isinstance(data, str):
            return hashlib.sha256(data.encode()).hexdigest()[:16]
        return data
    
    def log_security_event(self, event_type, user_id, details=None):
        """è¨˜éŒ„å®‰å…¨äº‹ä»¶"""
        sanitized_details = {}
        if details:
            for key, value in details.items():
                if key in ['user_id', 'email', 'ip_address']:
                    sanitized_details[key] = self.hash_pii(str(value))
                else:
                    sanitized_details[key] = self.sanitize_message(str(value))
        
        self.logger.warning(
            f"Security event: {event_type}",
            extra={
                "event_type": event_type,
                "user_id": self.hash_pii(user_id),
                "details": sanitized_details,
                "service": self.service_name
            }
        )
    
    def log_audit_event(self, action, resource, user_id, before_state=None, after_state=None):
        """è¨˜éŒ„å¯©è¨ˆäº‹ä»¶"""
        self.logger.info(
            f"Audit: {action} on {resource}",
            extra={
                "event_type": "audit",
                "action": action,
                "resource": resource,
                "user_id": self.hash_pii(user_id),
                "before_state": before_state,
                "after_state": after_state,
                "service": self.service_name
            }
        )

# ä½¿ç”¨å®‰å…¨æ—¥èªŒè¨˜éŒ„å™¨
secure_logger = SecureLogger("auth-service")

secure_logger.log_security_event(
    "login_attempt",
    "user@example.com",
    {"ip_address": "192.168.1.100", "user_agent": "Mozilla/5.0..."}
)

secure_logger.log_audit_event(
    "update_profile",
    "user_profile",
    "user@example.com",
    before_state={"name": "John"},
    after_state={"name": "John Doe"}
)
```

## ğŸš¨ éŒ¯èª¤è™•ç†èˆ‡æ¢å¾©

### å®¹éŒ¯æ©Ÿåˆ¶

```python
# fault_tolerance.py
import time
import threading
from queue import Queue, Empty
from pretty_loguru import create_logger

class FaultTolerantLogger:
    """å®¹éŒ¯æ—¥èªŒè¨˜éŒ„å™¨"""
    
    def __init__(self, service_name, backup_enabled=True):
        self.service_name = service_name
        self.backup_enabled = backup_enabled
        self.failed_logs = Queue()
        self.retry_thread = None
        
        # ä¸»è¦æ—¥èªŒè¨˜éŒ„å™¨
        try:
            self.primary_logger = create_logger(
                name=f"primary_{service_name}",
                level="INFO",
                log_path=f"/var/log/{service_name}/{service_name}.log",
                rotation="100 MB",
                retention="30 days",
                compression="gzip",
                enqueue=True
            )
        except Exception as e:
            print(f"ä¸»è¦æ—¥èªŒè¨˜éŒ„å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            self.primary_logger = None
        
        # å‚™ä»½æ—¥èªŒè¨˜éŒ„å™¨ï¼ˆè¨˜æ†¶é«”æˆ–å‚™ç”¨ä½ç½®ï¼‰
        if backup_enabled:
            try:
                self.backup_logger = create_logger(
                    name=f"backup_{service_name}",
                    level="INFO",
                    log_path=f"/tmp/{service_name}_backup.log",
                    rotation="50 MB",
                    retention="7 days"
                )
            except Exception as e:
                print(f"å‚™ä»½æ—¥èªŒè¨˜éŒ„å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                self.backup_logger = None
        
        # å•Ÿå‹•é‡è©¦æ©Ÿåˆ¶
        self._start_retry_mechanism()
    
    def log(self, level, message, **kwargs):
        """å®¹éŒ¯æ—¥èªŒè¨˜éŒ„"""
        log_entry = {
            "level": level,
            "message": message,
            "extra": kwargs,
            "timestamp": time.time()
        }
        
        try:
            # å˜—è©¦ä½¿ç”¨ä¸»è¦æ—¥èªŒè¨˜éŒ„å™¨
            if self.primary_logger:
                getattr(self.primary_logger, level.lower())(message, **kwargs)
                return True
        except Exception as e:
            print(f"ä¸»è¦æ—¥èªŒè¨˜éŒ„å¤±æ•—: {e}")
            # å°‡å¤±æ•—çš„æ—¥èªŒåŠ å…¥é‡è©¦ä½‡åˆ—
            self.failed_logs.put(log_entry)
        
        try:
            # ä½¿ç”¨å‚™ä»½æ—¥èªŒè¨˜éŒ„å™¨
            if self.backup_logger:
                getattr(self.backup_logger, level.lower())(
                    f"[BACKUP] {message}",
                    **kwargs
                )
                return True
        except Exception as e:
            print(f"å‚™ä»½æ—¥èªŒè¨˜éŒ„å¤±æ•—: {e}")
        
        # æ‰€æœ‰æ—¥èªŒè¨˜éŒ„å™¨éƒ½å¤±æ•—ï¼Œè¼¸å‡ºåˆ°æ§åˆ¶å°
        print(f"[{level}] {message} | Extra: {kwargs}")
        return False
    
    def info(self, message, **kwargs):
        return self.log("INFO", message, **kwargs)
    
    def error(self, message, **kwargs):
        return self.log("ERROR", message, **kwargs)
    
    def warning(self, message, **kwargs):
        return self.log("WARNING", message, **kwargs)
    
    def _start_retry_mechanism(self):
        """å•Ÿå‹•é‡è©¦æ©Ÿåˆ¶"""
        def retry_worker():
            while True:
                try:
                    # å¾å¤±æ•—ä½‡åˆ—ä¸­å–å‡ºæ—¥èªŒ
                    log_entry = self.failed_logs.get(timeout=30)
                    
                    # é‡è©¦è¨˜éŒ„æ—¥èªŒ
                    if self.primary_logger:
                        try:
                            getattr(self.primary_logger, log_entry["level"].lower())(
                                f"[RETRY] {log_entry['message']}",
                                **log_entry["extra"]
                            )
                        except Exception:
                            # é‡è©¦å¤±æ•—ï¼Œé‡æ–°åŠ å…¥ä½‡åˆ—ï¼ˆé™åˆ¶é‡è©¦æ¬¡æ•¸ï¼‰
                            retry_count = log_entry.get("retry_count", 0)
                            if retry_count < 3:
                                log_entry["retry_count"] = retry_count + 1
                                self.failed_logs.put(log_entry)
                
                except Empty:
                    # æ²’æœ‰å¾…é‡è©¦çš„æ—¥èªŒï¼Œç¹¼çºŒç­‰å¾…
                    continue
                except Exception as e:
                    print(f"é‡è©¦æ©Ÿåˆ¶éŒ¯èª¤: {e}")
        
        self.retry_thread = threading.Thread(target=retry_worker, daemon=True)
        self.retry_thread.start()

# ä½¿ç”¨å®¹éŒ¯æ—¥èªŒè¨˜éŒ„å™¨
fault_tolerant_logger = FaultTolerantLogger("critical-service")

# å³ä½¿åœ¨ç£ç¢Ÿç©ºé–“ä¸è¶³æˆ–æ¬Šé™å•é¡Œæ™‚ä¹Ÿèƒ½è¨˜éŒ„æ—¥èªŒ
fault_tolerant_logger.info("æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•", version="2.0.0")
fault_tolerant_logger.error("è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤", error_code=500, user_id="user123")
```

## ğŸ“‹ éƒ¨ç½²æª¢æŸ¥æ¸…å–®

### éƒ¨ç½²å‰æª¢æŸ¥

```bash
#!/bin/bash
# pre_deployment_check.sh

echo "ğŸ” ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æª¢æŸ¥"
echo "===================="

# 1. æª¢æŸ¥ç£ç¢Ÿç©ºé–“
echo "1. æª¢æŸ¥ç£ç¢Ÿç©ºé–“..."
df -h /var/log
if [ $(df /var/log | tail -1 | awk '{print $5}' | sed 's/%//') -gt 80 ]; then
    echo "âŒ è­¦å‘Šï¼šæ—¥èªŒç£ç¢Ÿç©ºé–“ä½¿ç”¨ç‡è¶…é 80%"
else
    echo "âœ… ç£ç¢Ÿç©ºé–“å……è¶³"
fi

# 2. æª¢æŸ¥æ—¥èªŒç›®éŒ„æ¬Šé™
echo "2. æª¢æŸ¥æ—¥èªŒç›®éŒ„æ¬Šé™..."
if [ -w /var/log/app ]; then
    echo "âœ… æ—¥èªŒç›®éŒ„å¯å¯«"
else
    echo "âŒ è­¦å‘Šï¼šæ—¥èªŒç›®éŒ„ä¸å¯å¯«"
fi

# 3. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
echo "3. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸..."
required_vars=("LOG_LEVEL" "ENVIRONMENT" "SERVICE_NAME")
for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "âŒ è­¦å‘Šï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ $var"
    else
        echo "âœ… $var = ${!var}"
    fi
done

# 4. æª¢æŸ¥æ—¥èªŒè¼ªæ›é…ç½®
echo "4. æª¢æŸ¥ logrotate é…ç½®..."
if [ -f /etc/logrotate.d/app ]; then
    echo "âœ… logrotate é…ç½®å­˜åœ¨"
else
    echo "âŒ è­¦å‘Šï¼šç¼ºå°‘ logrotate é…ç½®"
fi

# 5. æ¸¬è©¦æ—¥èªŒå¯«å…¥
echo "5. æ¸¬è©¦æ—¥èªŒå¯«å…¥..."
python3 -c "
from pretty_loguru import create_logger
try:
    logger = create_logger('test', log_path='/var/log/app/test.log')
    logger.info('éƒ¨ç½²æ¸¬è©¦')
    print('âœ… æ—¥èªŒå¯«å…¥æ¸¬è©¦æˆåŠŸ')
except Exception as e:
    print(f'âŒ æ—¥èªŒå¯«å…¥æ¸¬è©¦å¤±æ•—: {e}')
"

echo "===================="
echo "éƒ¨ç½²æª¢æŸ¥å®Œæˆ"
```

### ç”Ÿç”¢ç’°å¢ƒé…ç½®ç¯„æœ¬

```python
# production_template.py
"""
ç”Ÿç”¢ç’°å¢ƒé…ç½®ç¯„æœ¬
é©ç”¨æ–¼å¤§å¤šæ•¸ Web æ‡‰ç”¨ç¨‹å¼å’Œå¾®æœå‹™
"""

import os
from pretty_loguru import create_logger

class ProductionConfig:
    """ç”Ÿç”¢ç’°å¢ƒé…ç½®é¡"""
    
    def __init__(self):
        self.service_name = os.getenv("SERVICE_NAME", "app")
        self.environment = os.getenv("ENVIRONMENT", "production")
        self.log_level = os.getenv("LOG_LEVEL", "INFO")
        self.log_path = os.getenv("LOG_PATH", f"/var/log/{self.service_name}")
        
    def create_application_logger(self):
        """å»ºç«‹æ‡‰ç”¨ç¨‹å¼æ—¥èªŒè¨˜éŒ„å™¨"""
        return create_logger(
            name=f"{self.service_name}_app",
            level=self.log_level,
            log_path=f"{self.log_path}/application.log",
            rotation="100 MB",
            retention="30 days",
            compression="gzip",
            enqueue=True,
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name} | {function}:{line} - {message}",
            catch=True
        )
    
    def create_access_logger(self):
        """å»ºç«‹è¨ªå•æ—¥èªŒè¨˜éŒ„å™¨"""
        return create_logger(
            name=f"{self.service_name}_access",
            level="INFO",
            log_path=f"{self.log_path}/access.log",
            rotation="daily",
            retention="90 days",
            compression="gzip",
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {message}",
            serialize=True
        )
    
    def create_error_logger(self):
        """å»ºç«‹éŒ¯èª¤æ—¥èªŒè¨˜éŒ„å™¨"""
        return create_logger(
            name=f"{self.service_name}_error",
            level="WARNING",
            log_path=f"{self.log_path}/error.log",
            rotation="50 MB",
            retention="90 days",
            compression="gzip",
            backtrace=True,
            diagnose=True
        )
    
    def create_audit_logger(self):
        """å»ºç«‹å¯©è¨ˆæ—¥èªŒè¨˜éŒ„å™¨"""
        return create_logger(
            name=f"{self.service_name}_audit",
            level="INFO",
            log_path=f"{self.log_path}/audit.log",
            rotation="daily",
            retention="2555 days",  # 7å¹´åˆè¦è¦æ±‚
            compression="gzip",
            format='{"timestamp": "{time:YYYY-MM-DD HH:mm:ss.SSS}", "service": "' + self.service_name + '", "event": "audit", "message": "{message}", "data": {extra}}',
            serialize=True
        )

# ä½¿ç”¨é…ç½®
config = ProductionConfig()
app_logger = config.create_application_logger()
access_logger = config.create_access_logger()
error_logger = config.create_error_logger()
audit_logger = config.create_audit_logger()

# è¨˜éŒ„ä¸åŒé¡å‹çš„æ—¥èªŒ
app_logger.info("æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å®Œæˆ")
access_logger.info("ç”¨æˆ¶è¨ªå•", extra={"user_id": "123", "endpoint": "/api/users"})
error_logger.error("è³‡æ–™åº«é€£æ¥å¤±æ•—", extra={"database": "postgres", "error": "timeout"})
audit_logger.info("ç”¨æˆ¶è³‡æ–™æ›´æ–°", extra={"user_id": "123", "action": "update_profile"})
```

## ğŸ”— ç›¸é—œè³‡æº

- [æ•ˆèƒ½æœ€ä½³åŒ–](./performance) - æ€§èƒ½èª¿å„ªæŒ‡å—
- [è‡ªå®šç¾©é…ç½®](./custom-config) - è©³ç´°é…ç½®é¸é …
- [æ—¥èªŒè¼ªæ›](./log-rotation) - æª”æ¡ˆç®¡ç†ç­–ç•¥
- [ç¯„ä¾‹é›†åˆ](../examples/production/) - ç”Ÿç”¢ç’°å¢ƒç¯„ä¾‹