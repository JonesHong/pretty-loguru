#!/usr/bin/env python3
"""
ğŸ“Š 08_enterprise/monitoring_integration.py
ç›£æ§ç³»çµ±æ•´åˆç¯„ä¾‹

é€™å€‹ç¯„ä¾‹å±•ç¤ºäº†å¦‚ä½•å°‡ pretty-loguru èˆ‡ä¼æ¥­ç´šç›£æ§ç³»çµ±æ•´åˆï¼Œ
åŒ…å« Prometheus æŒ‡æ¨™ã€Grafana å„€è¡¨æ¿ã€ELK Stack å’Œå ±è­¦é€šçŸ¥ç³»çµ±ã€‚
"""

import time
import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, deque
from dataclasses import dataclass
from pretty_loguru import create_logger

# æ¨¡æ“¬ Prometheus å®¢æˆ¶ç«¯ï¼ˆåœ¨å¯¦éš›ç’°å¢ƒä¸­ä½¿ç”¨ prometheus_clientï¼‰
class PrometheusMetrics:
    """Prometheus æŒ‡æ¨™æ¨¡æ“¬é¡"""
    
    def __init__(self):
        self.counters = defaultdict(int)
        self.histograms = defaultdict(list)
        self.gauges = defaultdict(float)
    
    def counter_inc(self, name: str, labels: Dict[str, str] = None):
        """è¨ˆæ•¸å™¨å¢åŠ """
        key = f"{name}:{json.dumps(labels or {}, sort_keys=True)}"
        self.counters[key] += 1
    
    def histogram_observe(self, name: str, value: float, labels: Dict[str, str] = None):
        """ç›´æ–¹åœ–è§€å¯Ÿå€¼"""
        key = f"{name}:{json.dumps(labels or {}, sort_keys=True)}"
        self.histograms[key].append(value)
    
    def gauge_set(self, name: str, value: float, labels: Dict[str, str] = None):
        """é‡è¡¨è¨­ç½®å€¼"""
        key = f"{name}:{json.dumps(labels or {}, sort_keys=True)}"
        self.gauges[key] = value
    
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å–æ‰€æœ‰æŒ‡æ¨™"""
        return {
            "counters": dict(self.counters),
            "histograms": {k: {
                "count": len(v),
                "sum": sum(v),
                "avg": sum(v) / len(v) if v else 0,
                "percentiles": self._calculate_percentiles(v)
            } for k, v in self.histograms.items()},
            "gauges": dict(self.gauges)
        }
    
    def _calculate_percentiles(self, values: List[float]) -> Dict[str, float]:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        if not values:
            return {}
        
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        return {
            "p50": sorted_values[int(n * 0.5)],
            "p90": sorted_values[int(n * 0.9)],
            "p95": sorted_values[int(n * 0.95)],
            "p99": sorted_values[int(n * 0.99)] if n > 1 else sorted_values[0]
        }

@dataclass
class ElasticsearchDocument:
    """Elasticsearch æ–‡æª”æ¨¡å‹"""
    index: str
    doc_type: str
    body: Dict[str, Any]
    timestamp: datetime

class ElasticsearchClient:
    """Elasticsearch å®¢æˆ¶ç«¯æ¨¡æ“¬é¡"""
    
    def __init__(self):
        self.documents = []
        self.indices = set()
    
    def index(self, index: str, doc_type: str, body: Dict[str, Any]):
        """ç´¢å¼•æ–‡æª”"""
        doc = ElasticsearchDocument(
            index=index,
            doc_type=doc_type,
            body=body,
            timestamp=datetime.utcnow()
        )
        self.documents.append(doc)
        self.indices.add(index)
    
    def search(self, index: str, query: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """æœå°‹æ–‡æª”"""
        results = []
        for doc in self.documents:
            if doc.index == index:
                results.append({
                    "_index": doc.index,
                    "_type": doc.doc_type,
                    "_source": doc.body,
                    "@timestamp": doc.timestamp.isoformat()
                })
        return results[:100]  # é™åˆ¶è¿”å›çµæœæ•¸é‡
    
    def get_indices(self) -> List[str]:
        """ç²å–æ‰€æœ‰ç´¢å¼•"""
        return list(self.indices)

class AlertManager:
    """å ±è­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alerts = []
        self.alert_rules = {}
        self.notification_channels = []
    
    def add_alert_rule(self, name: str, condition: str, severity: str, 
                       threshold: float = None):
        """æ·»åŠ å ±è­¦è¦å‰‡"""
        self.alert_rules[name] = {
            "condition": condition,
            "severity": severity,
            "threshold": threshold,
            "created_at": datetime.utcnow().isoformat()
        }
    
    def trigger_alert(self, rule_name: str, message: str, details: Dict[str, Any] = None):
        """è§¸ç™¼å ±è­¦"""
        if rule_name not in self.alert_rules:
            return
        
        alert = {
            "id": f"alert_{len(self.alerts) + 1}",
            "rule_name": rule_name,
            "message": message,
            "severity": self.alert_rules[rule_name]["severity"],
            "details": details or {},
            "triggered_at": datetime.utcnow().isoformat(),
            "status": "firing"
        }
        
        self.alerts.append(alert)
        self._send_notification(alert)
    
    def _send_notification(self, alert: Dict[str, Any]):
        """ç™¼é€é€šçŸ¥"""
        print(f"ğŸš¨ ALERT [{alert['severity'].upper()}]: {alert['message']}")
        print(f"   Rule: {alert['rule_name']}")
        print(f"   Time: {alert['triggered_at']}")

class MonitoringIntegration:
    """ç›£æ§ç³»çµ±æ•´åˆå™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–å„ç¨®ç›£æ§çµ„ä»¶
        self.prometheus = PrometheusMetrics()
        self.elasticsearch = ElasticsearchClient()
        self.alert_manager = AlertManager()
        
        # æ•ˆèƒ½æŒ‡æ¨™è¿½è¹¤
        self.performance_metrics = {
            "log_processing_times": deque(maxlen=1000),
            "log_volume": defaultdict(int),
            "error_rates": defaultdict(int),
            "response_times": deque(maxlen=1000)
        }
        
        # å»ºç«‹ç›£æ§æ—¥èªŒè¨˜éŒ„å™¨
        self.logger = create_logger(
            name="monitoring_integration",
            log_path="logs/monitoring",
            level="INFO",
            rotation="hourly",
            retention="30 days"
        )
        
        # è¨­ç½®å ±è­¦è¦å‰‡
        self._setup_alert_rules()
        
        self.logger.info("ğŸ“Š ç›£æ§ç³»çµ±æ•´åˆå™¨å•Ÿå‹•")
    
    def _setup_alert_rules(self):
        """è¨­ç½®å ±è­¦è¦å‰‡"""
        self.alert_manager.add_alert_rule(
            name="high_error_rate",
            condition="error_rate > 5%",
            severity="critical",
            threshold=0.05
        )
        
        self.alert_manager.add_alert_rule(
            name="slow_response_time",
            condition="avg_response_time > 2s",
            severity="warning",
            threshold=2.0
        )
        
        self.alert_manager.add_alert_rule(
            name="log_volume_spike",
            condition="log_volume > 1000/min",
            severity="warning",
            threshold=1000
        )
    
    def track_log_event(self, level: str, service: str, duration: float = None):
        """è¿½è¹¤æ—¥èªŒäº‹ä»¶"""
        # æ›´æ–° Prometheus æŒ‡æ¨™
        self.prometheus.counter_inc("logs_total", {"level": level, "service": service})
        
        if duration is not None:
            self.prometheus.histogram_observe("log_processing_duration_seconds", duration, {"service": service})
            self.performance_metrics["log_processing_times"].append(duration)
        
        # æ›´æ–°å…§éƒ¨æŒ‡æ¨™
        self.performance_metrics["log_volume"][service] += 1
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è§¸ç™¼å ±è­¦
        self._check_volume_alerts(service)
    
    def track_application_metrics(self, service: str, response_time: float, 
                                error_occurred: bool = False):
        """è¿½è¹¤æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™"""
        # è¨˜éŒ„å›æ‡‰æ™‚é–“
        self.prometheus.histogram_observe("http_request_duration_seconds", response_time, {"service": service})
        self.performance_metrics["response_times"].append(response_time)
        
        # è¨˜éŒ„éŒ¯èª¤ç‡
        if error_occurred:
            self.prometheus.counter_inc("http_requests_errors_total", {"service": service})
            self.performance_metrics["error_rates"][service] += 1
        
        # è¨˜éŒ„ç¸½è«‹æ±‚æ•¸
        self.prometheus.counter_inc("http_requests_total", {"service": service})
        
        # æª¢æŸ¥æ•ˆèƒ½å ±è­¦
        self._check_performance_alerts(service, response_time, error_occurred)
    
    def send_to_elasticsearch(self, log_record: Dict[str, Any], index_prefix: str = "logs"):
        """ç™¼é€æ—¥èªŒåˆ° Elasticsearch"""
        # æ ¼å¼åŒ–ç‚º ELK æ¨™æº–æ ¼å¼
        elk_document = {
            "@timestamp": datetime.utcnow().isoformat(),
            "@version": "1",
            "message": log_record.get("message", ""),
            "level": log_record.get("level", "INFO"),
            "logger": log_record.get("name", "default"),
            "service": log_record.get("service", "unknown"),
            "host": "localhost",
            "fields": log_record.get("extra", {}),
            "tags": ["pretty-loguru", "enterprise"],
            "timestamp": log_record.get("time", datetime.utcnow().isoformat())
        }
        
        # æ ¹æ“šæ—¥æœŸå»ºç«‹ç´¢å¼•
        index_name = f"{index_prefix}-{datetime.utcnow().strftime('%Y.%m.%d')}"
        
        try:
            self.elasticsearch.index(
                index=index_name,
                doc_type="_doc",
                body=elk_document
            )
            
            self.track_log_event("INFO", "elasticsearch")
            
        except Exception as e:
            self.logger.error(f"ç™¼é€åˆ° Elasticsearch å¤±æ•—: {e}")
    
    def _check_volume_alerts(self, service: str):
        """æª¢æŸ¥æ—¥èªŒé‡å ±è­¦"""
        current_volume = self.performance_metrics["log_volume"][service]
        
        # ç°¡å–®çš„å ±è­¦é‚è¼¯ï¼šæ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
        if current_volume > 100:  # æ¨¡æ“¬é–¾å€¼
            self.alert_manager.trigger_alert(
                rule_name="log_volume_spike",
                message=f"æœå‹™ {service} æ—¥èªŒé‡ç•°å¸¸å¢åŠ ",
                details={"service": service, "volume": current_volume}
            )
    
    def _check_performance_alerts(self, service: str, response_time: float, error_occurred: bool):
        """æª¢æŸ¥æ•ˆèƒ½å ±è­¦"""
        # æª¢æŸ¥å›æ‡‰æ™‚é–“
        if response_time > 2.0:
            self.alert_manager.trigger_alert(
                rule_name="slow_response_time",
                message=f"æœå‹™ {service} å›æ‡‰æ™‚é–“éæ…¢",
                details={"service": service, "response_time": response_time}
            )
        
        # æª¢æŸ¥éŒ¯èª¤ç‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if error_occurred:
            error_count = self.performance_metrics["error_rates"][service]
            if error_count > 5:  # ç°¡å–®é–¾å€¼
                self.alert_manager.trigger_alert(
                    rule_name="high_error_rate",
                    message=f"æœå‹™ {service} éŒ¯èª¤ç‡éé«˜",
                    details={"service": service, "error_count": error_count}
                )
    
    def create_grafana_dashboard_config(self) -> Dict[str, Any]:
        """å»ºç«‹ Grafana å„€è¡¨æ¿é…ç½®"""
        dashboard_config = {
            "dashboard": {
                "id": None,
                "title": "Pretty-Loguru Enterprise Monitoring",
                "description": "ä¼æ¥­ç´šæ—¥èªŒç›£æ§å„€è¡¨æ¿",
                "tags": ["logging", "monitoring", "enterprise"],
                "timezone": "UTC",
                "refresh": "30s",
                "time": {
                    "from": "now-1h",
                    "to": "now"
                },
                "panels": [
                    {
                        "id": 1,
                        "title": "Log Volume by Service",
                        "type": "graph",
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
                        "targets": [
                            {
                                "expr": "rate(logs_total[5m])",
                                "legendFormat": "{{service}} - {{level}}",
                                "refId": "A"
                            }
                        ],
                        "yAxes": [
                            {"label": "Logs per second", "min": 0}
                        ]
                    },
                    {
                        "id": 2,
                        "title": "Log Processing Time",
                        "type": "graph",
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
                        "targets": [
                            {
                                "expr": "histogram_quantile(0.95, log_processing_duration_seconds)",
                                "legendFormat": "95th percentile",
                                "refId": "A"
                            },
                            {
                                "expr": "histogram_quantile(0.50, log_processing_duration_seconds)",
                                "legendFormat": "50th percentile",
                                "refId": "B"
                            }
                        ]
                    },
                    {
                        "id": 3,
                        "title": "Error Rate by Service",
                        "type": "singlestat",
                        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 8},
                        "targets": [
                            {
                                "expr": "rate(http_requests_errors_total[5m]) / rate(http_requests_total[5m]) * 100",
                                "refId": "A"
                            }
                        ],
                        "valueName": "current",
                        "format": "percent",
                        "thresholds": "1,5"
                    },
                    {
                        "id": 4,
                        "title": "Active Alerts",
                        "type": "table",
                        "gridPos": {"h": 6, "w": 12, "x": 0, "y": 12},
                        "targets": [
                            {
                                "expr": "ALERTS",
                                "refId": "A"
                            }
                        ]
                    }
                ]
            }
        }
        
        self.logger.info("ğŸ“Š Grafana å„€è¡¨æ¿é…ç½®å·²å»ºç«‹", extra={"dashboard_panels": len(dashboard_config["dashboard"]["panels"])})
        return dashboard_config
    
    def generate_monitoring_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç›£æ§å ±å‘Š"""
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        processing_times = list(self.performance_metrics["log_processing_times"])
        response_times = list(self.performance_metrics["response_times"])
        
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        
        report = {
            "report_timestamp": datetime.utcnow().isoformat(),
            "monitoring_period": "last_hour",
            "metrics_summary": {
                "total_logs_processed": sum(self.performance_metrics["log_volume"].values()),
                "avg_log_processing_time_ms": round(avg_processing_time * 1000, 2),
                "avg_response_time_ms": round(avg_response_time * 1000, 2),
                "total_errors": sum(self.performance_metrics["error_rates"].values()),
                "elasticsearch_indices": len(self.elasticsearch.get_indices()),
                "active_alerts": len([a for a in self.alert_manager.alerts if a["status"] == "firing"])
            },
            "prometheus_metrics": self.prometheus.get_metrics(),
            "top_services_by_volume": dict(sorted(
                self.performance_metrics["log_volume"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]),
            "recent_alerts": self.alert_manager.alerts[-10:],  # æœ€è¿‘10å€‹å ±è­¦
            "health_status": "healthy" if avg_response_time < 1.0 else "degraded"
        }
        
        self.logger.info("ğŸ“ˆ ç›£æ§å ±å‘Šç”Ÿæˆå®Œæˆ", extra=report)
        return report

async def simulate_monitored_application():
    """æ¨¡æ“¬è¢«ç›£æ§çš„æ‡‰ç”¨ç¨‹å¼"""
    monitoring = MonitoringIntegration()
    
    print("ğŸ“Š ç›£æ§ç³»çµ±æ•´åˆæ¼”ç¤º")
    print("=" * 50)
    
    # æ¨¡æ“¬å„ç¨®æ‡‰ç”¨ç¨‹å¼æ´»å‹•
    services = ["user-service", "order-service", "payment-service", "notification-service"]
    
    for i in range(50):  # æ¨¡æ“¬50å€‹è«‹æ±‚
        service = services[i % len(services)]
        
        # æ¨¡æ“¬è«‹æ±‚è™•ç†
        start_time = time.time()
        
        # æ¨¡æ“¬ä¸€äº›è«‹æ±‚æœƒæ¯”è¼ƒæ…¢
        processing_time = 0.1 + (0.5 if i % 10 == 0 else 0)
        await asyncio.sleep(processing_time)
        
        response_time = time.time() - start_time
        
        # æ¨¡æ“¬ä¸€äº›è«‹æ±‚æœƒå¤±æ•—
        error_occurred = i % 15 == 0
        
        # è¿½è¹¤æŒ‡æ¨™
        monitoring.track_application_metrics(service, response_time, error_occurred)
        monitoring.track_log_event("INFO" if not error_occurred else "ERROR", service, processing_time)
        
        # ç™¼é€æ—¥èªŒåˆ° Elasticsearch
        log_record = {
            "message": f"è™•ç†è«‹æ±‚ {i+1}",
            "level": "ERROR" if error_occurred else "INFO",
            "service": service,
            "request_id": f"req_{i+1}",
            "response_time": response_time,
            "extra": {
                "user_id": f"user_{(i % 10) + 1}",
                "endpoint": f"/api/{service.split('-')[0]}",
                "status_code": 500 if error_occurred else 200
            }
        }
        
        monitoring.send_to_elasticsearch(log_record)
        
        # æ¯10å€‹è«‹æ±‚è¼¸å‡ºä¸€æ¬¡é€²åº¦
        if (i + 1) % 10 == 0:
            print(f"   è™•ç†äº† {i + 1} å€‹è«‹æ±‚...")
    
    # ç”Ÿæˆ Grafana å„€è¡¨æ¿é…ç½®
    print("\nğŸ“Š å»ºç«‹ Grafana å„€è¡¨æ¿...")
    dashboard_config = monitoring.create_grafana_dashboard_config()
    print(f"   å„€è¡¨æ¿åŒ…å« {len(dashboard_config['dashboard']['panels'])} å€‹é¢æ¿")
    
    # ç”Ÿæˆç›£æ§å ±å‘Š
    print("\nğŸ“ˆ ç”Ÿæˆç›£æ§å ±å‘Š...")
    report = monitoring.generate_monitoring_report()
    print(f"   è™•ç†æ—¥èªŒç¸½æ•¸: {report['metrics_summary']['total_logs_processed']}")
    print(f"   å¹³å‡è™•ç†æ™‚é–“: {report['metrics_summary']['avg_log_processing_time_ms']} ms")
    print(f"   å¹³å‡å›æ‡‰æ™‚é–“: {report['metrics_summary']['avg_response_time_ms']} ms")
    print(f"   éŒ¯èª¤ç¸½æ•¸: {report['metrics_summary']['total_errors']}")
    print(f"   æ´»èºå ±è­¦æ•¸: {report['metrics_summary']['active_alerts']}")
    print(f"   ç³»çµ±å¥åº·ç‹€æ…‹: {report['health_status']}")
    
    print("\nğŸ“ ç›£æ§æ—¥èªŒæª”æ¡ˆä½ç½®:")
    print("   - logs/monitoring/ (ç›£æ§ç³»çµ±æ—¥èªŒ)")
    
    return monitoring

if __name__ == "__main__":
    asyncio.run(simulate_monitored_application())