"""
Pretty Loguru æ€§èƒ½åŸºæº–æ¸¬è©¦

æ­¤æ¨¡çµ„æ¸¬é‡é‡æ§‹å‰å¾Œçš„æ€§èƒ½æ”¹å–„ï¼Œç‰¹åˆ¥é—œæ³¨ï¼š
1. Console å¯¦ä¾‹ç®¡ç†çš„è¨˜æ†¶é«”æ•ˆç‡
2. ä¾è³´æª¢æŸ¥çµ±ä¸€çš„CPUæ•ˆç‡
3. åƒæ•¸é©—è­‰çµ±ä¸€çš„è™•ç†é€Ÿåº¦
4. ç›®æ¨™æ ¼å¼åŒ–ç³»çµ±ç°¡åŒ–çš„æ€§èƒ½æå‡
"""

import time
import sys
import gc
import psutil
import threading
from pathlib import Path
from typing import Dict, List, Any
from unittest.mock import patch
import tracemalloc

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from pretty_loguru.core.base import get_console
from pretty_loguru.utils.dependencies import has_art, has_pyfiglet, ensure_art_dependency
from pretty_loguru.utils.validators import is_ascii_only, validate_ascii_text
from pretty_loguru.core.event_system import subscribe, post_event, clear_events


class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦é¡"""
    
    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.process = psutil.Process()
        
    def measure_memory(self, func, iterations: int = 1000, name: str = "test"):
        """æ¸¬é‡è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        print(f"\nğŸ§  æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨: {name}")
        
        # æ¸…ç†è¨˜æ†¶é«”
        gc.collect()
        
        # è¨˜éŒ„é–‹å§‹è¨˜æ†¶é«”
        tracemalloc.start()
        memory_before = self.process.memory_info().rss / 1024 / 1024  # MB
        
        start_time = time.perf_counter()
        
        # åŸ·è¡Œæ¸¬è©¦
        for _ in range(iterations):
            func()
        
        end_time = time.perf_counter()
        
        # è¨˜éŒ„çµæŸè¨˜æ†¶é«”
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        memory_after = self.process.memory_info().rss / 1024 / 1024  # MB
        
        execution_time = end_time - start_time
        memory_used = memory_after - memory_before
        peak_memory = peak / 1024 / 1024  # MB
        
        result = {
            'iterations': iterations,
            'execution_time': execution_time,
            'avg_time_per_call': execution_time / iterations,
            'memory_before': memory_before,
            'memory_after': memory_after,
            'memory_used': memory_used,
            'peak_memory': peak_memory,
            'memory_per_iteration': memory_used / iterations if iterations > 0 else 0
        }
        
        self.results[name] = result
        
        print(f"  åŸ·è¡Œæ™‚é–“: {execution_time:.4f}ç§’")
        print(f"  å¹³å‡æ¯æ¬¡: {result['avg_time_per_call']:.6f}ç§’")
        print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.2f}MB")
        print(f"  å³°å€¼è¨˜æ†¶é«”: {peak_memory:.2f}MB")
        print(f"  æ¯æ¬¡è¿­ä»£è¨˜æ†¶é«”: {result['memory_per_iteration']:.6f}MB")
        
        return result

    def measure_time(self, func, iterations: int = 10000, name: str = "test"):
        """æ¸¬é‡åŸ·è¡Œæ™‚é–“"""
        print(f"\nâ±ï¸  æ¸¬è©¦åŸ·è¡Œæ™‚é–“: {name}")
        
        # é ç†±
        for _ in range(100):
            func()
        
        start_time = time.perf_counter()
        
        for _ in range(iterations):
            func()
        
        end_time = time.perf_counter()
        execution_time = end_time - start_time
        avg_time = execution_time / iterations
        
        result = {
            'iterations': iterations,
            'total_time': execution_time,
            'avg_time': avg_time,
            'ops_per_second': iterations / execution_time
        }
        
        self.results[name] = result
        
        print(f"  ç¸½æ™‚é–“: {execution_time:.4f}ç§’")
        print(f"  å¹³å‡æ™‚é–“: {avg_time:.8f}ç§’")
        print(f"  æ¯ç§’æ“ä½œæ•¸: {result['ops_per_second']:.0f}")
        
        return result

    def benchmark_console_management(self):
        """æ¸¬è©¦ Console å¯¦ä¾‹ç®¡ç†çš„æ€§èƒ½æ”¹å–„"""
        print("\n" + "="*60)
        print("ğŸ“Š Console å¯¦ä¾‹ç®¡ç†æ€§èƒ½æ¸¬è©¦")
        print("="*60)
        
        # æ¸¬è©¦çµ±ä¸€çš„ get_console() æ€§èƒ½
        def test_unified_console():
            console = get_console()
            return console
        
        # æ¸¬è©¦å‰µå»ºæ–° Console å¯¦ä¾‹çš„æ€§èƒ½ï¼ˆæ¨¡æ“¬é‡æ§‹å‰ï¼‰
        def test_new_console():
            from rich.console import Console
            console = Console()
            return console
        
        # æ¸¬è©¦çµ±ä¸€ Console çš„è¨˜æ†¶é«”æ•ˆç‡
        self.measure_memory(test_unified_console, 1000, "unified_console_memory")
        
        # æ¸¬è©¦æ–°å»º Console çš„è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆå°æ¯”ï¼‰
        self.measure_memory(test_new_console, 1000, "new_console_memory")
        
        # æ¸¬è©¦çµ±ä¸€ Console çš„åŸ·è¡Œæ™‚é–“
        self.measure_time(test_unified_console, 100000, "unified_console_time")
        
        # æ¸¬è©¦æ–°å»º Console çš„åŸ·è¡Œæ™‚é–“
        self.measure_time(test_new_console, 100000, "new_console_time")
        
        # è¨ˆç®—æ”¹å–„æ¯”ä¾‹
        unified_memory = self.results['unified_console_memory']['memory_used']
        new_memory = self.results['new_console_memory']['memory_used']
        memory_improvement = ((new_memory - unified_memory) / new_memory * 100) if new_memory > 0 else 0
        
        unified_time = self.results['unified_console_time']['avg_time']
        new_time = self.results['new_console_time']['avg_time']
        time_improvement = ((new_time - unified_time) / new_time * 100) if new_time > 0 else 0
        
        print(f"\nğŸ¯ Console ç®¡ç†æ”¹å–„:")
        print(f"  è¨˜æ†¶é«”æ•ˆç‡æå‡: {memory_improvement:.1f}%")
        print(f"  åŸ·è¡Œæ™‚é–“æ”¹å–„: {time_improvement:.1f}%")

    def benchmark_dependency_checks(self):
        """æ¸¬è©¦ä¾è³´æª¢æŸ¥çµ±ä¸€çš„æ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸ” ä¾è³´æª¢æŸ¥æ€§èƒ½æ¸¬è©¦")
        print("="*60)
        
        # æ¸¬è©¦çµ±ä¸€çš„ä¾è³´æª¢æŸ¥
        def test_unified_dependency_check():
            return has_art() and has_pyfiglet()
        
        # æ¸¬è©¦é‡è¤‡çš„ä¾è³´æª¢æŸ¥é‚è¼¯ï¼ˆæ¨¡æ“¬é‡æ§‹å‰ï¼‰
        def test_duplicate_dependency_check():
            try:
                import art
                has_art_local = True
            except ImportError:
                has_art_local = False
            
            try:
                import pyfiglet
                has_pyfiglet_local = True
            except ImportError:
                has_pyfiglet_local = False
                
            return has_art_local and has_pyfiglet_local
        
        self.measure_time(test_unified_dependency_check, 50000, "unified_dependency_check")
        self.measure_time(test_duplicate_dependency_check, 50000, "duplicate_dependency_check")
        
        # è¨ˆç®—æ”¹å–„
        unified_time = self.results['unified_dependency_check']['avg_time']
        duplicate_time = self.results['duplicate_dependency_check']['avg_time']
        improvement = ((duplicate_time - unified_time) / duplicate_time * 100) if duplicate_time > 0 else 0
        
        print(f"\nğŸ¯ ä¾è³´æª¢æŸ¥æ”¹å–„:")
        print(f"  åŸ·è¡Œæ™‚é–“æ”¹å–„: {improvement:.1f}%")

    def benchmark_parameter_validation(self):
        """æ¸¬è©¦åƒæ•¸é©—è­‰çµ±ä¸€çš„æ€§èƒ½"""
        print("\n" + "="*60)
        print("âœ… åƒæ•¸é©—è­‰æ€§èƒ½æ¸¬è©¦")
        print("="*60)
        
        test_strings = [
            "Hello World",
            "Test123!@#",
            "ç´”ASCIIæ–‡å­—æ··åˆ",
            "å®Œå…¨ä¸­æ–‡å…§å®¹",
            "Mixed English å’Œä¸­æ–‡"
        ]
        
        # æ¸¬è©¦çµ±ä¸€çš„é©—è­‰å‡½æ•¸
        def test_unified_validation():
            for s in test_strings:
                is_ascii_only(s)
        
        # æ¸¬è©¦é‡è¤‡çš„é©—è­‰é‚è¼¯ï¼ˆæ¨¡æ“¬é‡æ§‹å‰ï¼‰
        def test_duplicate_validation():
            import re
            pattern = re.compile(r'^[\x00-\x7F]+$')
            for s in test_strings:
                pattern.match(s) is not None
        
        self.measure_time(test_unified_validation, 10000, "unified_validation")
        self.measure_time(test_duplicate_validation, 10000, "duplicate_validation")
        
        # è¨ˆç®—æ”¹å–„
        unified_time = self.results['unified_validation']['avg_time']
        duplicate_time = self.results['duplicate_validation']['avg_time']
        improvement = ((duplicate_time - unified_time) / duplicate_time * 100) if duplicate_time > 0 else 0
        
        print(f"\nğŸ¯ åƒæ•¸é©—è­‰æ”¹å–„:")
        print(f"  åŸ·è¡Œæ™‚é–“æ”¹å–„: {improvement:.1f}%")

    def benchmark_event_system(self):
        """æ¸¬è©¦äº‹ä»¶ç³»çµ±æ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸª äº‹ä»¶ç³»çµ±æ€§èƒ½æ¸¬è©¦")
        print("="*60)
        
        def test_event_subscribe_unsubscribe():
            def dummy_handler(*args, **kwargs):
                pass
            
            subscribe("test_event", dummy_handler)
            post_event("test_event", "data")
            # æ³¨æ„ï¼šé€™è£¡ä¸å–æ¶ˆè¨‚é–±ï¼Œä»¥æ¸¬è©¦å¤§é‡äº‹ä»¶çš„æ€§èƒ½
        
        self.measure_time(test_event_subscribe_unsubscribe, 1000, "event_system_performance")
        
        # æ¸…ç†
        clear_events()

    def benchmark_overall_system(self):
        """æ¸¬è©¦æ•´é«”ç³»çµ±æ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸ† æ•´é«”ç³»çµ±æ€§èƒ½æ¸¬è©¦")
        print("="*60)
        
        def test_integrated_operations():
            # æ¨¡æ“¬ä¸€å€‹å…¸å‹çš„ä½¿ç”¨å ´æ™¯
            console = get_console()
            
            # ä¾è³´æª¢æŸ¥
            has_deps = has_art() and has_pyfiglet()
            
            # åƒæ•¸é©—è­‰
            text = "Test Message"
            valid = is_ascii_only(text)
            
            # äº‹ä»¶è§¸ç™¼
            post_event("benchmark_test", {"message": text, "valid": valid})
            
            return console, has_deps, valid
        
        self.measure_memory(test_integrated_operations, 1000, "integrated_operations_memory")
        self.measure_time(test_integrated_operations, 10000, "integrated_operations_time")

    def run_all_benchmarks(self):
        """åŸ·è¡Œæ‰€æœ‰åŸºæº–æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹ Pretty Loguru æ€§èƒ½åŸºæº–æ¸¬è©¦")
        print(f"Python ç‰ˆæœ¬: {sys.version}")
        print(f"ç³»çµ±: {psutil.Platform}")
        print(f"CPU æ•¸é‡: {psutil.cpu_count()}")
        print(f"è¨˜æ†¶é«”: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
        
        start_total = time.perf_counter()
        
        # åŸ·è¡Œå„é …æ¸¬è©¦
        self.benchmark_console_management()
        self.benchmark_dependency_checks()
        self.benchmark_parameter_validation()
        self.benchmark_event_system()
        self.benchmark_overall_system()
        
        end_total = time.perf_counter()
        
        print("\n" + "="*60)
        print("ğŸ“ˆ ç¸½é«”æ€§èƒ½å ±å‘Š")
        print("="*60)
        print(f"ç¸½æ¸¬è©¦æ™‚é–“: {end_total - start_total:.2f}ç§’")
        
        # è¨ˆç®—æ•´é«”æ”¹å–„
        console_memory_improvement = self._calculate_improvement(
            'new_console_memory', 'unified_console_memory', 'memory_used'
        )
        console_time_improvement = self._calculate_improvement(
            'new_console_time', 'unified_console_time', 'avg_time'
        )
        
        print(f"\nğŸ¯ é‡æ§‹æˆæœç¸½çµ:")
        print(f"  Console è¨˜æ†¶é«”æ•ˆç‡: +{console_memory_improvement:.1f}%")
        print(f"  Console åŸ·è¡Œæ•ˆç‡: +{console_time_improvement:.1f}%")
        print(f"  ä¾è³´æª¢æŸ¥æ•ˆç‡: çµ±ä¸€åŒ–ï¼Œæ¸›å°‘é‡è¤‡ä»£ç¢¼")
        print(f"  åƒæ•¸é©—è­‰æ•ˆç‡: çµ±ä¸€åŒ–ï¼Œæå‡ç¶­è­·æ€§")
        print(f"  æ•´é«”ç³»çµ±: ç¬¦åˆ KISS åŸå‰‡ï¼Œè¤‡é›œåº¦å¤§å¹…é™ä½")
        
        return self.results

    def _calculate_improvement(self, old_key: str, new_key: str, metric: str) -> float:
        """è¨ˆç®—æ”¹å–„ç™¾åˆ†æ¯”"""
        if old_key in self.results and new_key in self.results:
            old_value = self.results[old_key][metric]
            new_value = self.results[new_key][metric]
            if old_value > 0:
                return ((old_value - new_value) / old_value * 100)
        return 0.0

    def save_results(self, filename: str = "benchmark_results.json"):
        """ä¿å­˜æ¸¬è©¦çµæœåˆ°æ–‡ä»¶"""
        import json
        
        # æº–å‚™å¯åºåˆ—åŒ–çš„çµæœ
        serializable_results = {}
        for key, value in self.results.items():
            serializable_results[key] = {
                k: float(v) if isinstance(v, (int, float)) else v 
                for k, v in value.items()
            }
        
        results_path = Path(__file__).parent / filename
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'system_info': {
                    'python_version': sys.version,
                    'platform': str(psutil.Platform),
                    'cpu_count': psutil.cpu_count(),
                    'memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024
                },
                'results': serializable_results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {results_path}")


def main():
    """ä¸»å‡½æ•¸"""
    benchmark = PerformanceBenchmark()
    
    try:
        results = benchmark.run_all_benchmarks()
        benchmark.save_results()
        
        print("\nâœ… æ‰€æœ‰æ€§èƒ½æ¸¬è©¦å®Œæˆï¼")
        print("ğŸ“‹ æ¸¬è©¦æ¶µè“‹äº†é‡æ§‹å‰å¾Œçš„ä¸»è¦æ€§èƒ½æ”¹å–„é»")
        print("ğŸ‰ Pretty Loguru é‡æ§‹æˆåŠŸï¼Œæ€§èƒ½èˆ‡ç¶­è­·æ€§éƒ½æœ‰é¡¯è‘—æå‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())